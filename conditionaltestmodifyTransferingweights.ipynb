{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qd6CiVe-jyEn",
    "outputId": "b4187c26-21ff-4edc-e194-db58fd55f233"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 17:39:13.552957: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-06 17:39:13.552990: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#loading the libraries\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Concatenate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy import stats\n",
    "import rpy2\n",
    "from scipy.stats import spearmanr,pearsonr\n",
    "from CCIT import CCIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OxvAXLLFnl7d"
   },
   "outputs": [],
   "source": [
    "#defining the class MDN\n",
    "class MDN_module(tf.keras.Model):\n",
    "#changed from 15 to 20\n",
    "    def __init__(self, neurons=15, components = 1):\n",
    "        super(MDN_module, self).__init__(name=\"MDN_module\")\n",
    "        self.neurons = neurons\n",
    "        self.components = components\n",
    "\n",
    "        #chaging activation to relu from linear, changin relu to sigmoid \n",
    "        for i in range(1,3):\n",
    "          s=\"self\"+\".h\"+str(i)+\"= Dense(neurons, activation=\\\"relu\\\", name=\"+\"'h\"+str(i)+\"')\"\n",
    "          exec(s)\n",
    "        #self.h1=Dense(12,activation=\"relu\",name=\"h1\")\n",
    "        #self.h2=Dense(8,activation=\"relu\",name=\"h2\")\n",
    "        #self.h3=Dense(8,activation=\"relu\",name=\"h3\")\n",
    "        self.alphas = Dense(components, activation=\"softmax\", name=\"alphas\")\n",
    "        self.mus = Dense(components, activation=\"linear\",name=\"mus\") \n",
    "        self.sigmas = Dense(components, activation=\"nnelu\",name=\"sigmas\") #activation changed from linear to default\n",
    "        self.pvec = Concatenate(name=\"pvec\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x=self.h1(inputs)\n",
    "        #x=self.inputA(inputs)\n",
    "        x=self.h2(x)\n",
    "        #x=self.h3(x)\n",
    "        alpha_v = self.alphas(x)\n",
    "        mu_v = self.mus(x)\n",
    "        sigma_v = self.sigmas(x)\n",
    "        \n",
    "        return self.pvec([alpha_v,mu_v, sigma_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7DSPXmR1ogiH"
   },
   "outputs": [],
   "source": [
    "no_parameters=3\n",
    "components=1\n",
    "def nnelu(input):\n",
    "    \"\"\" Computes the Non-Negative Exponential Linear Unit\n",
    "    \"\"\"\n",
    "    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(input))\n",
    "\n",
    "def slice_parameter_vectors(parameter_vector):\n",
    "    \"\"\" Returns an unpacked list of paramter vectors.\n",
    "    \"\"\"\n",
    "    return [parameter_vector[:,i*components:(i+1\n",
    "    )*components] for i in range(no_parameters)]\n",
    "\n",
    "def gnll_loss(y, parameter_vector):\n",
    "    \"\"\" Computes the mean negative log-likelihood loss of y given the mixture parameters.\n",
    "    \"\"\"\n",
    "    alpha,mu,sigma = slice_parameter_vectors(parameter_vector) # Unpack parameter vectors\n",
    "    #tf.print(sigma)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "           mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "           components_distribution=tfd.Normal(\n",
    "           loc=mu,       \n",
    "           scale=sigma))\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_likelihood =  gm.log_prob(tf.transpose(y)) # Evaluate log-probability of y \n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1) \n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'nnelu': Activation(nnelu)})\n",
    "\n",
    "def gnll_eval(y,alpha, mu, sigma):\n",
    "    \"\"\" Computes the mean negative log-likelihood loss of y given the mixture parameters.\n",
    "    \"\"\"\n",
    "    #print(alpha)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "        components_distribution=tfd.Normal(\n",
    "            loc=mu,       \n",
    "            scale=sigma))\n",
    "    log_likelihood = gm.log_prob(tf.transpose(y))\n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "\n",
    "def eval_mdn_model(x_test, y_test, mdn_model):\n",
    "    \"\"\" Evaluate the model to get the loss for the given x and y \n",
    "    \"\"\"\n",
    "    y_pred = mdn_model.predict(np.reshape(x_test,newshape=(len(x_test),-1)))\n",
    "    alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    mdn_nll = gnll_eval(y_test.astype(np.float32),alpha, mu, sigma).numpy()\n",
    "    return mdn_nll\n",
    "#reshapefunction\n",
    "def eval_mdn_model_mle(x_test,y_test):\n",
    "        indices_1 = [i for i, x in enumerate(x_test) if x == 1]\n",
    "        #changing x to -1\n",
    "        indices_0 = [i for i, x in enumerate(x_test) if x == 0]\n",
    "        mu_0=np.mean(y_test[indices_0])\n",
    "        mu_1=np.mean(y_test[indices_1])\n",
    "        sigma_0=np.std(y_test[indices_0])\n",
    "        sigma_1=np.std(y_test[indices_1])\n",
    "        y_mean=np.zeros((len(y_test),1))\n",
    "        y_mean[indices_1]=mu_1\n",
    "        y_mean[indices_0]=mu_0\n",
    "        y_std=np.zeros((len(y_test),1))\n",
    "        y_std[indices_1]=sigma_1\n",
    "        y_std[indices_0]=sigma_0\n",
    "        alpha=np.ones((len(y_mean),1))\n",
    "        return gnll_eval(y_test,alpha,y_mean,y_std).numpy()\n",
    "    \n",
    "def reshapevar(X):\n",
    "  \"\"\"\n",
    "  Function to reshape the vector for the input \n",
    "  \"\"\"\n",
    "  return np.reshape(X,newshape=(len(X),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T6sJ7ffirsnB"
   },
   "outputs": [],
   "source": [
    "def compute_loss(P,Q,mle=False):\n",
    "    \"\"\" Compute the loss for the given pair\n",
    "    \"\"\"\n",
    "    if(mle==False):\n",
    "        opt = tf.optimizers.Adam(1e-2)\n",
    "        mdn_PQ = MDN_module()\n",
    "        mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "        mdn_PQ.fit(x=reshapevar(P), y=np.array(Q).T,epochs=100,  batch_size=64,verbose=0)\n",
    "        #return np.array(nlcor.nlcor(P,Q)[0])[0]\n",
    "        return eval_mdn_model(P,Q,mdn_PQ)\n",
    "    else:\n",
    "        return eval_mdn_model_mle(P,Q)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_y_pred(P,Q,mle=False):\n",
    "    \"\"\" Compute the loss for the given pair\n",
    "    \"\"\"\n",
    "    if(mle==False):\n",
    "        opt = tf.optimizers.Adam(1e-2)\n",
    "        mdn_PQ = MDN_module()\n",
    "        mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "        mdn_PQ.fit(x=reshapevar(P), y=np.array(Q).T,epochs=100,  batch_size=64,verbose=0)\n",
    "        y_pred = mdn_PQ.predict(np.reshape(P,newshape=(len(P),-1)))\n",
    "        return y_pred[:,1]\n",
    "    else:\n",
    "        indices_1 = [i for i, x in enumerate(P) if x == 1]\n",
    "        indices_0 = [i for i, x in enumerate(P) if x == 0]\n",
    "        mu_0=np.mean(Q[indices_0])\n",
    "        mu_1=np.mean(Q[indices_1])\n",
    "        #sigma_0=np.std(Q[indices_0])\n",
    "        #sigma_1=np.std(Q[indices_1])\n",
    "        y_mean=np.zeros((len(Q),1))\n",
    "        y_mean[indices_1]=mu_1\n",
    "        y_mean[indices_0]=mu_0\n",
    "        return y_mean.reshape((len(y_mean),))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X6V8X2QFpmC7"
   },
   "outputs": [],
   "source": [
    "def shuffleBtimes(P,Q,B,mle=False):\n",
    "    \"\"\" Shuffle Q B times and compute the loss \n",
    "    \"\"\"\n",
    "    loss=[]\n",
    "    if(mle==False):\n",
    "        for i in range(0,B):\n",
    "          loss.append(compute_loss(P,np.random.permutation(Q)))\n",
    "    else:\n",
    "        for i in range(0,B):\n",
    "          loss.append(compute_loss(P,np.random.permutation(Q),True))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iUW3ikOHu6PO"
   },
   "outputs": [],
   "source": [
    "def LinearLABData():\n",
    "    \"\"\" Generate the linear data \n",
    "    \"\"\"\n",
    "    L = np.random.binomial(1,0.5,1000)  \n",
    "    beta0 = np.ones(1000)-0.4\n",
    "    #beta1 = 0.5\n",
    "    beta1=3\n",
    "    beta2= 0.3\n",
    "    beta3=0.8\n",
    "    #eps0 = np.random.standard_normal(1000)\n",
    "    #eps1 = np.random.standard_normal(1000)\n",
    "    eps0 = np.random.normal(0,1,1000)\n",
    "    eps1 = np.random.normal(0,1,1000)\n",
    "    A = beta0 + beta1*L + eps0\n",
    "    #B=beta2+beta3*np.sin(A)+eps1\n",
    "    B = beta2+ beta3*A + eps1 \n",
    "    plt.scatter(A,B)\n",
    "    plt.title(\"A vs B\")\n",
    "    plt.xlabel(\"A\")\n",
    "    plt.ylabel(\"B\")\n",
    "    return [L,A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo=open(\"/home/aravind/projects/CIT_Non_Linear/LinearDifferentvalues/testing_writingvalues_Linear0to1.txt\", \"r\")\n",
    "L=[]\n",
    "A=[]\n",
    "B=[]\n",
    "#fe=open(\"dataset_params.txt\",'w')\n",
    "for i in range(0,121):\n",
    "    line=fo.readline()\n",
    "    #fe.write(line)\n",
    "    #line=line[1:-2] #remove double quotes \n",
    "    #param = [j for j in line.split()]\n",
    "    #print(param)\n",
    "    #chrname.append(param[1])\n",
    "    #g1.append(param[2])\n",
    "    #g2.append(param[3])\n",
    "    line=fo.readline()\n",
    "    l = [j for j in line.split()]\n",
    "    L.append([int(i) for i in l])\n",
    "    line=fo.readline()\n",
    "    a = [j for j in line.split()]\n",
    "    A.append([float(i) for i in a])\n",
    "    line=fo.readline()\n",
    "    b = [j for j in line.split()]\n",
    "    B.append([float(i) for i in b])\n",
    "dataset_linear = [i for i in zip(L,A,B)]\n",
    "fo.close()\n",
    "#fe.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pvalue(original,loss_list):\n",
    "    '''\n",
    "    calculate the p value \n",
    "    '''\n",
    "    return sum(i < original for i in loss_list)/len(loss_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_third_testloss(A,B):\n",
    "    opt = tf.optimizers.Adam(1e-2)\n",
    "    mdn_PQ = MDN_module()\n",
    "    mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "##changing epochs didnt make much difference\n",
    "\n",
    "#mdn_PQ.fit(x=C, y=np.array(B).T,epochs=300,  batch_size=64)\n",
    "    withoutL=mdn_PQ.fit(x=A, y=B.T,epochs=100,  batch_size=64,verbose=0)\n",
    "    y_pred = mdn_PQ.predict(A)\n",
    "    alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "        components_distribution=tfd.Normal(\n",
    "            loc=mu,       \n",
    "            scale=sigma))\n",
    "    log_likelihood = gm.log_prob(B).numpy()\n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_difference(L,A,B):\n",
    "    return compute_third_testloss(reshapevar(A),np.array(B))-compute_third_testloss(np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1),np.array(B))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_B_n_times_diff(L,A,B,n):\n",
    "    loss=[]\n",
    "    indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "    #changing x== -1 \n",
    "    indices_0 = [i for i, x in enumerate(L) if x == 0]\n",
    "    for i in range(0,n):\n",
    "        B_dist_temp=np.zeros(len(B))\n",
    "        mod_indices_1=random.sample(indices_1,len(indices_1))\n",
    "        for i in range(len(indices_1)):\n",
    "            B_dist_temp[indices_1[i]]=B[mod_indices_1[i]]\n",
    "\n",
    "        mod_indices_0=random.sample(indices_0,len(indices_0))\n",
    "        for i in range(len(indices_0)):\n",
    "            B_dist_temp[indices_0[i]]=B[mod_indices_0[i]]\n",
    "        loss.append(calculate_difference(L,A,B_dist_temp))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=84\n",
    "A=np.array(dataset_linear[i][1])\n",
    "B=np.array(dataset_linear[i][2])\n",
    "L=np.array(dataset_linear[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0,144):\n",
    "A=np.array(dataset_linear[i][1])\n",
    "B=np.array(dataset_linear[i][2])\n",
    "L=np.array(dataset_linear[i][0])\n",
    "shuffles=100\n",
    "A_shuffle=np.copy(A)\n",
    "B_shuffle=np.copy(B)\n",
    "#print(\"Original\",B_shufflep\n",
    "#changed the second test from mle to using neural networks\n",
    "loss_list_LA=shuffleBtimes(L,A_shuffle,shuffles,True)\n",
    "loss_list_LB=shuffleBtimes(L,B_shuffle,shuffles,True)\n",
    "#loss_list_Bresidual=stratify_B_n_times_diff(L,A_shuffle,B_shuffle,shuffles) #conditional independence test\n",
    "\n",
    "true_LA=compute_loss(L,A,True)\n",
    "true_LB=compute_loss(L,B,True)\n",
    "loss_list_Bresidual,true_LBresidual= calculateLshuffle(L,A,B,shuffles)\n",
    "#true_LBresidual=stratify_B_ntimes_permuteL(L,A,B,shuffles)\n",
    "#true_LBresidual=calculate_difference(L,A,B)\n",
    "#true_LBresidual=trueconditional(L,A,B)\n",
    "#loss_list_Bresidual=stratify_B_ntimes_permuteL(L,A_shuffle,B_shuffle,shuffles)\n",
    "#loss_list_Bresidual=stratify_B_ntimes_permuteL(L,A,B,shuffles,True)\n",
    "LA_p=calculate_pvalue(true_LA,loss_list_LA)\n",
    "LB_p=calculate_pvalue(true_LB,loss_list_LB)\n",
    "AB_p=calculate_pvalue(true_LBresidual,loss_list_Bresidual)\n",
    "f.write(str(i)+\",\"+str(LA_p)+\",\"+str(LB_p)+\",\"+str(AB_p)+\"\\n\")\n",
    "#AB_p=calculate_pvalue(true_LBresidual,loss_list_Bresidual)\n",
    "#mean_true=np.mean(true_LBresidual)\n",
    "#mean_shuffle=np.mean(loss_list_Bresidual)\n",
    "#std_true=np.std(true_LBresidual)\n",
    "#std_shuffle=np.std(loss_list_Bresidual)\n",
    "#f.write(str(i)+\",\"+str(LA_p)+\",\"+str(LB_p)+\",\"+str(mean_true)+\",\"+str(mean_shuffle)+\",\"+str(std_true)+\",\"+str(std_shuffle)+\"\\n\")\n",
    "#pickle_items=[loss_list_LA,loss_list_LB,loss_list_Bresidual,true_LA,true_LB,true_LBresidual,LA_p,LB_p,AB_p]\n",
    "#file_name=str(dataset_names[i])+\".pkl\"\n",
    "#open_file = open(\"./DLresultspickle/\"+file_name, \"wb\")\n",
    "#pickle.dump(pickle_items, open_file)\n",
    "#open_file.close()\n",
    "\n",
    "#if(mean_shuffle>mean_true):\n",
    "    #print(i)\n",
    "    #count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LA_p=calculate_pvalue(true_LA,loss_list_LA)\n",
    "LB_p=calculate_pvalue(true_LB,loss_list_LB)\n",
    "AB_p=calculate_pvalue(true_LBresidual,loss_list_Bresidual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the original LA->B and then shuffle L and calculate the loss, we are retraining the model \n",
    "#at every shuffle here\n",
    "def calculateLshuffle(L,A,B,shuffle):\n",
    "    loss_list=[]\n",
    "    opt = tf.optimizers.SGD(1e-3)\n",
    "    mdn_PQ = MDN_module()\n",
    "    mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "    withoutL=mdn_PQ.fit(x=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1), y=B.T,epochs=100,  batch_size=32,verbose=0)\n",
    "    y_pred = mdn_PQ.predict(np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1))\n",
    "    alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "    mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "    components_distribution=tfd.Normal(\n",
    "    loc=mu,       \n",
    "    scale=sigma))\n",
    "    log_likelihood = gm.log_prob(B).numpy()\n",
    "    orig_loss= -tf.reduce_mean(log_likelihood, axis=-1).numpy()\n",
    "    for i in range(shuffle):\n",
    "        L_shuffle=np.random.permutation(L)\n",
    "        opt = tf.optimizers.SGD(1e-3)\n",
    "        mdn_PQ = MDN_module()\n",
    "        mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "        withoutL=mdn_PQ.fit(x=np.concatenate([L_shuffle.reshape(-1,1),A.reshape(-1,1)],axis=1), y=B.T,epochs=100,  batch_size=32,verbose=0)\n",
    "        y_pred = mdn_PQ.predict(np.concatenate([L_shuffle.reshape(-1,1),A.reshape(-1,1)],axis=1))\n",
    "        alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "        gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "        components_distribution=tfd.Normal(\n",
    "        loc=mu,       \n",
    "        scale=sigma))\n",
    "        log_likelihood = gm.log_prob(B).numpy()\n",
    "        loss= -tf.reduce_mean(log_likelihood, axis=-1).numpy()\n",
    "        loss_list.append(loss)\n",
    "    \n",
    "    return loss_list,loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=84\n",
    "A=np.array(dataset_linear[i][1])\n",
    "B=np.array(dataset_linear[i][2])\n",
    "L=np.array(dataset_linear[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(L,A,B,mdn_PQ):\n",
    "    weightofa=mdn_PQ.layers[0].get_weights()[0][0]\n",
    "    #get the bias\n",
    "    biasweight=mdn_PQ.layers[0].get_weights()[1]\n",
    "    #set the weights of L as zero \n",
    "    weightofL=np.zeros(15,dtype=np.float32)\n",
    "    #combine the weights of L and A\n",
    "    combinedweight=np.array([weightofL,weightofa])\n",
    "    #combine the weights and biases\n",
    "    layer0weightrans=[combinedweight,biasweight]\n",
    "    #only for one epoch just to get the weights \n",
    "    opt = tf.optimizers.SGD(1e-3)\n",
    "    mdn_trans = MDN_module()\n",
    "    mdn_trans.compile(loss=gnll_loss, optimizer=opt)\n",
    "    mdn_trans.fit(x=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1), y=B.T,epochs=1,  batch_size=32,verbose=0)\n",
    "    y_pred = mdn_trans.predict(np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1))\n",
    "\n",
    "\n",
    "    mdn_trans.layers[0].set_weights(layer0weightrans)\n",
    "\n",
    "    a=mdn_PQ.layers[1].get_weights()\n",
    "    mdn_trans.layers[1].set_weights(a)  \n",
    "    a=mdn_PQ.layers[2].get_weights()\n",
    "    mdn_trans.layers[2].set_weights(a)  \n",
    "    a=mdn_PQ.layers[3].get_weights()\n",
    "    mdn_trans.layers[3].set_weights(a)  \n",
    "    a=mdn_PQ.layers[4].get_weights()\n",
    "    mdn_trans.layers[4].set_weights(a)  \n",
    "    a=mdn_PQ.layers[5].get_weights()\n",
    "    mdn_trans.layers[5].set_weights(a)  \n",
    "    return mdn_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train the model given the inputs\n",
    "def modeltrain(x,y,weights=None):\n",
    "    opt = tf.optimizers.SGD(1e-3)\n",
    "    \n",
    "    mdn_PQ = MDN_module()\n",
    "    mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "    if(weights!=None):\n",
    "        mdn_PQ.fit(x, y,epochs=1,batch_size = batchsize,verbose=0)\n",
    "        for i in range(0,6):\n",
    "            mdn_PQ.layers[i].set_weights(weights[i])\n",
    "            \n",
    "    #calculate the prediction and the loss \n",
    "    mdn_PQ.fit(x, y,epochs=epochs,batch_size=batchsize ,verbose=0)\n",
    "    y_pred = mdn_PQ.predict(x)\n",
    "    alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "            mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "            components_distribution=tfd.Normal(\n",
    "                loc=mu,       \n",
    "                scale=sigma))\n",
    "    log_likelihood = gm.log_prob(y).numpy()\n",
    "    loss_o = -tf.reduce_mean(log_likelihood, axis=-1).numpy()\n",
    "    return mdn_PQ,loss_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate the weights \n",
    "def calculateweights(mdn_PQ):\n",
    "    weights=[]\n",
    "    weightofa=mdn_PQ.layers[0].get_weights()[0][0]\n",
    "    #get the bias\n",
    "    biasweight=mdn_PQ.layers[0].get_weights()[1]\n",
    "    #set the weights of L as zero \n",
    "    weightofL=np.zeros(15,dtype=np.float32)\n",
    "    #combine the weights of L and A\n",
    "    combinedweight=np.array([weightofL,weightofa])\n",
    "    #combine the weights and biases\n",
    "    layer0weightrans=[combinedweight,biasweight]\n",
    "    weights.append(layer0weightrans)\n",
    "    for i in range(1,6):\n",
    "        a=mdn_PQ.layers[i].get_weights()\n",
    "        weights.append(a)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to stratify the data\n",
    "def stratifydata(L,B):\n",
    "    indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "    #changin x==0 to x=-1\n",
    "    indices_0 = [i for i, x in enumerate(L) if x == 0]\n",
    "    B_dist_temp=np.zeros(len(B))\n",
    "    mod_indices_1=random.sample(indices_1,len(indices_1))\n",
    "    for i in range(len(indices_1)):\n",
    "        B_dist_temp[indices_1[i]]=B[mod_indices_1[i]]\n",
    "\n",
    "    mod_indices_0=random.sample(indices_0,len(indices_0))\n",
    "    for i in range(len(indices_0)):\n",
    "        B_dist_temp[indices_0[i]]=B[mod_indices_0[i]]\n",
    "    return B_dist_temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split the data\n",
    "def splitdata(L,A,B):\n",
    "    Aalone_L1=[]\n",
    "    Aalone_L0=[]\n",
    "    Balone_L1=[]\n",
    "    Balone_L0=[]\n",
    "    for i in range(len(L)):\n",
    "        if(L[i]==1):\n",
    "            Aalone_L1.append(A[i])\n",
    "            Balone_L1.append(B[i])\n",
    "        else:\n",
    "            Aalone_L0.append(A[i])\n",
    "            Balone_L0.append(B[i])\n",
    "    \n",
    "    return [np.array(Aalone_L1),np.array(Aalone_L0),np.array(Balone_L0),np.array(Balone_L1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs=100\n",
    "batchsize=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking one cycle \n",
    "def thirdtestonecycle(L,A,B):\n",
    "    loss_Aalone=[]\n",
    "    loss_AL=[]\n",
    "    halfdata= splitdata(L,A,B)\n",
    "    trainthirdtest(L,A,B,halfdata,loss_Aalone,loss_AL)\n",
    "    trainthirdtest(L,A,B,halfdata,loss_Aalone,loss_AL,True,half=0)\n",
    "    trainthirdtest(L,A,B,halfdata,loss_Aalone,loss_AL,True,half=1)\n",
    "    trainthirdtest(L,A,B,halfdata,loss_Aalone,loss_AL,True)\n",
    "    print(\"Loss with A alone: Loss A\",loss_Aalone)\n",
    "    print(\"Loss with A and L: Loss LA\",loss_AL)\n",
    "    a= 1 if min(loss_Aalone)-min(loss_AL) >=0 else 0\n",
    "    return min(loss_Aalone)-min(loss_AL),a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirdtestoriginal(L,A,B):\n",
    "    print(\"Original data\")\n",
    "    print(\"Loss Order: Random,L=0 tranfer, L= 1 tranfer, Full data transfer\")\n",
    "    loss,neg=thirdtestonecycle(L,A,B)\n",
    "    print(\" Difference(minimum from Loss A - minimum from Loss LA) \",loss)\n",
    "    return loss,neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thirdtestperm(L,A,B,n):\n",
    "    loss_perm=[]\n",
    "    for i in range(0,n):\n",
    "        print(\"Permutation \",i)\n",
    "        print(\"Loss Order: Random,L=0 tranfer, L= 1 tranfer, Full data transfer\")\n",
    "        B_perm=stratifydata(L,B)\n",
    "        loss,neg=thirdtestonecycle(L,A,B_perm)\n",
    "        print(\" Difference(minimum from Loss A - minimum from Loss LA) \",loss)\n",
    "        loss_perm.append(loss)\n",
    "    return loss_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data\n",
      "Loss Order: Random,L=0 tranfer, L= 1 tranfer, Full data transfer\n",
      "Loss with A alone: Loss A [1.422357, 1.5076702, 1.4936984, 1.4412404]\n",
      "Loss with A and L: Loss LA [1.4783486, 1.4304905, 1.4258473, 1.4244506]\n",
      " Difference(minimum from Loss A - minimum from Loss LA)  -0.0020936728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.0020936728, 0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thirdtestoriginal(L,A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation  0\n",
      "Loss Order: Random,L=0 tranfer, L= 1 tranfer, Full data transfer\n",
      "Loss with A alone: Loss A [1.7182806, 1.6734651, 1.7376497, 1.7190794]\n",
      "Loss with A and L: Loss LA [1.7078007, 1.7164137, 1.7135024, 1.7132818]\n",
      " Difference(minimum from Loss A - minimum from Loss LA)  -0.034335613\n",
      "Permutation  1\n",
      "Loss Order: Random,L=0 tranfer, L= 1 tranfer, Full data transfer\n",
      "Loss with A alone: Loss A [1.7188756, 1.666958, 1.7497087, 1.7265329]\n",
      "Loss with A and L: Loss LA [1.7114409, 1.7163256, 1.7184064, 1.7177016]\n",
      " Difference(minimum from Loss A - minimum from Loss LA)  -0.044482946\n",
      "Permutation  2\n",
      "Loss Order: Random,L=0 tranfer, L= 1 tranfer, Full data transfer\n",
      "Loss with A alone: Loss A [1.716985, 1.660557, 1.7423065, 1.7167656]\n",
      "Loss with A and L: Loss LA [1.7083428, 1.7198298, 1.715471, 1.7154375]\n",
      " Difference(minimum from Loss A - minimum from Loss LA)  -0.04778576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.034335613, -0.044482946, -0.04778576]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thirdtestperm(L,A,B,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train the data for each configuration \n",
    "def trainthirdtest(L,A,B,halfdata,loss_Aalone,loss_AL,transfer=False,half=None):\n",
    "    AL=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1)\n",
    "    if half==None:\n",
    "        mdn,loss=modeltrain(reshapevar(A),B.T)\n",
    "        loss_Aalone.append(loss)\n",
    "        weights=None\n",
    "        if transfer==True :\n",
    "            weights=calculateweights(mdn)\n",
    "        mdn,loss=modeltrain(AL,B.T,weights)\n",
    "        loss_AL.append(loss)\n",
    "    elif half==1:\n",
    "        Aalone_L1=halfdata[0]\n",
    "        Balone_L1=halfdata[3]\n",
    "        mdn,loss=modeltrain(reshapevar(Aalone_L1),Balone_L1.T)\n",
    "        loss_Aalone.append(loss)\n",
    "        weights=None\n",
    "        if transfer==True :\n",
    "            weights=calculateweights(mdn)\n",
    "        mdn,loss=modeltrain(AL,B.T,weights)\n",
    "        loss_AL.append(loss)\n",
    "    elif half==0:\n",
    "        Aalone_L0=halfdata[1]\n",
    "        Balone_L0=halfdata[2]\n",
    "        mdn,loss=modeltrain(reshapevar(Aalone_L0),Balone_L0.T)\n",
    "        loss_Aalone.append(loss)\n",
    "        weights=None\n",
    "        if transfer==True :\n",
    "            weights=calculateweights(mdn)\n",
    "        mdn,loss=modeltrain(AL,B.T,weights)\n",
    "        loss_AL.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferweights(L,A,B,n):\n",
    "    #train the A->B model\n",
    "    opt = tf.optimizers.SGD(1e-3)\n",
    "    mdn_PQ = MDN_module()\n",
    "    #calculate the prediction and the loss \n",
    "    mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "    withoutL=mdn_PQ.fit(x=reshapevar(A), y=B.T,epochs=100,batch_size=32,verbose=0)\n",
    "    y_pred = mdn_PQ.predict(reshapevar(A))\n",
    "    alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "            mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "            components_distribution=tfd.Normal(\n",
    "                loc=mu,       \n",
    "                scale=sigma))\n",
    "    log_likelihood = gm.log_prob(B).numpy()\n",
    "    loss_o = -tf.reduce_mean(log_likelihood, axis=-1).numpy()\n",
    "    #get the weights of A\n",
    "    weightofa=mdn_PQ.layers[0].get_weights()[0][0]\n",
    "    #get the bias\n",
    "    biasweight=mdn_PQ.layers[0].get_weights()[1]\n",
    "    #set the weights of L as zero \n",
    "    weightofL=np.zeros(15,dtype=np.float32)\n",
    "    #combine the weights of L and A\n",
    "    combinedweight=np.array([weightofL,weightofa])\n",
    "    #combine the weights and biases\n",
    "    layer0weightrans=[combinedweight,biasweight]\n",
    "    #only for one epoch just to get the weights \n",
    "    opt = tf.optimizers.SGD(1e-3)\n",
    "    mdn_trans = MDN_module()\n",
    "    mdn_trans.compile(loss=gnll_loss, optimizer=opt)\n",
    "    mdn_trans.fit(x=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1), y=B.T,epochs=1,  batch_size=32,verbose=0)\n",
    "    y_pred = mdn_trans.predict(np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1))\n",
    "\n",
    "\n",
    "    mdn_trans.layers[0].set_weights(layer0weightrans)\n",
    "\n",
    "    a=mdn_PQ.layers[1].get_weights()\n",
    "    mdn_trans.layers[1].set_weights(a)  \n",
    "    a=mdn_PQ.layers[2].get_weights()\n",
    "    mdn_trans.layers[2].set_weights(a)  \n",
    "    a=mdn_PQ.layers[3].get_weights()\n",
    "    mdn_trans.layers[3].set_weights(a)  \n",
    "    a=mdn_PQ.layers[4].get_weights()\n",
    "    mdn_trans.layers[4].set_weights(a)  \n",
    "    a=mdn_PQ.layers[5].get_weights()\n",
    "    mdn_trans.layers[5].set_weights(a)  \n",
    "    #after setting the weights train it for 100 epochs with L also as input\n",
    "    mdn_trans.fit(x=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1), y=B.T,epochs=100,  batch_size=32,verbose=0)\n",
    "    y_pred = mdn_trans.predict(np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1))\n",
    "    alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "            mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "            components_distribution=tfd.Normal(\n",
    "                loc=mu,       \n",
    "                scale=sigma))\n",
    "    log_likelihood = gm.log_prob(B).numpy()\n",
    "    loss_o_L= -tf.reduce_mean(log_likelihood, axis=-1).numpy()\n",
    "    trueloss= loss_o-loss_o_L\n",
    "\n",
    "    loss=[]\n",
    "    indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "    #changin x==0 to x=-1\n",
    "    indices_0 = [i for i, x in enumerate(L) if x == 0]\n",
    "    for i in range(0,n):\n",
    "        B_dist_temp=np.zeros(len(B))\n",
    "        mod_indices_1=random.sample(indices_1,len(indices_1))\n",
    "        for i in range(len(indices_1)):\n",
    "            B_dist_temp[indices_1[i]]=B[mod_indices_1[i]]\n",
    "\n",
    "        mod_indices_0=random.sample(indices_0,len(indices_0))\n",
    "        for i in range(len(indices_0)):\n",
    "            B_dist_temp[indices_0[i]]=B[mod_indices_0[i]]\n",
    "\n",
    "\n",
    "        #train A-> Bstratified \n",
    "        opt = tf.optimizers.SGD(1e-3)\n",
    "        mdn_PQ = MDN_module()\n",
    "\n",
    "        mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "        mdn_PQ.fit(x=reshapevar(A), y=B_dist_temp.T,epochs=100,batch_size=32,verbose=0)\n",
    "        y_pred = mdn_PQ.predict(reshapevar(A))\n",
    "\n",
    "        alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "        gm = tfd.MixtureSameFamily(\n",
    "                mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "                components_distribution=tfd.Normal(\n",
    "                    loc=mu,       \n",
    "                    scale=sigma))\n",
    "        log_likelihood = gm.log_prob(B_dist_temp).numpy()\n",
    "        loss_o_strat= -tf.reduce_mean(log_likelihood, axis=-1).numpy()\n",
    "\n",
    "        #get the weights of A\n",
    "        weightofa=mdn_PQ.layers[0].get_weights()[0][0]\n",
    "        #get the bias\n",
    "        biasweight=mdn_PQ.layers[0].get_weights()[1]\n",
    "        #set the weights of L as zero \n",
    "        weightofL=np.zeros(15,dtype=np.float32)\n",
    "        #combine the weights of L and A\n",
    "        combinedweight=np.array([weightofL,weightofa])\n",
    "        #combine the weights and biases\n",
    "        layer0weightrans=[combinedweight,biasweight]\n",
    "\n",
    "        #only for one epoch just to get the weights \n",
    "        opt = tf.optimizers.SGD(1e-3)\n",
    "        mdn_trans = MDN_module()\n",
    "        mdn_trans.compile(loss=gnll_loss, optimizer=opt)\n",
    "        mdn_trans.fit(x=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1), y=B_dist_temp.T,epochs=1,  batch_size=32,verbose=0)\n",
    "        y_pred = mdn_trans.predict(np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1))\n",
    "\n",
    "\n",
    "        mdn_trans.layers[0].set_weights(layer0weightrans)\n",
    "\n",
    "        a=mdn_PQ.layers[1].get_weights()\n",
    "        mdn_trans.layers[1].set_weights(a)  \n",
    "        a=mdn_PQ.layers[2].get_weights()\n",
    "        mdn_trans.layers[2].set_weights(a)  \n",
    "        a=mdn_PQ.layers[3].get_weights()\n",
    "        mdn_trans.layers[3].set_weights(a)  \n",
    "        a=mdn_PQ.layers[4].get_weights()\n",
    "        mdn_trans.layers[4].set_weights(a)  \n",
    "        a=mdn_PQ.layers[5].get_weights()\n",
    "        mdn_trans.layers[5].set_weights(a)  \n",
    "        #train it for 100 epochs after setting the weights \n",
    "\n",
    "        mdn_trans.fit(x=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1), y=B_dist_temp.T,epochs=100,  batch_size=32,verbose=0)\n",
    "        y_pred = mdn_trans.predict(np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1))\n",
    "\n",
    "        alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "        gm = tfd.MixtureSameFamily(\n",
    "                mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "                components_distribution=tfd.Normal(\n",
    "                    loc=mu,       \n",
    "                    scale=sigma))\n",
    "        log_likelihood = gm.log_prob(B_dist_temp).numpy()\n",
    "        loss_o_strat_L= -tf.reduce_mean(log_likelihood, axis=-1).numpy()\n",
    "        loss.append(loss_o_strat-loss_o_strat_L)\n",
    "    return trueloss,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "t,l=transferweights(L,A,B,n)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CITNonLinear.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
