{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Concatenate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "import random\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pickle\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from scipy import stats\n",
    "from sklearn.svm import SVR\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#First argument for the number of runs, second for the initial run\n",
    "j=int(sys.argv[2])\n",
    "n=int(sys.argv[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"NLMRSVMFILinear.txt\",\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the class MDN\n",
    "class MDN_module(tf.keras.Model):\n",
    "    def __init__(self, neurons=15, components = 1):\n",
    "        super(MDN_module, self).__init__(name=\"MDN_module\")\n",
    "        self.neurons = neurons\n",
    "        self.components = components\n",
    "\n",
    "        #chaging activation to relu from linear, changin relu to sigmoid \n",
    "        for i in range(1,3):\n",
    "          s=\"self\"+\".h\"+str(i)+\"= Dense(neurons, activation=\\\"relu\\\", name=\"+\"'h\"+str(i)+\"')\"\n",
    "          exec(s)\n",
    "        #self.h1=Dense(12,activation=\"relu\",name=\"h1\")\n",
    "        #self.h2=Dense(8,activation=\"relu\",name=\"h2\")\n",
    "        #self.h3=Dense(8,activation=\"relu\",name=\"h3\")\n",
    "        self.alphas = Dense(components, activation=\"softmax\", name=\"alphas\")\n",
    "        self.mus = Dense(components, activation=\"linear\",name=\"mus\") \n",
    "        self.sigmas = Dense(components, activation=\"nnelu\",name=\"sigmas\") #activation changed from linear to default\n",
    "        self.pvec = Concatenate(name=\"pvec\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x=self.h1(inputs)\n",
    "        #x=self.inputA(inputs)\n",
    "        #x=self.h2(x)\n",
    "        #x=self.h3(x)\n",
    "        alpha_v = self.alphas(x)\n",
    "        mu_v = self.mus(x)\n",
    "        sigma_v = self.sigmas(x)\n",
    "        \n",
    "        return self.pvec([alpha_v,mu_v, sigma_v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nelu function, function to calculate the losses and evaluate them\n",
    "no_parameters=3\n",
    "components=1\n",
    "#change this variable for x == 0 or x == -1\n",
    "leftside=0\n",
    "#activation function\n",
    "def nnelu(input):\n",
    "    \"\"\" Computes the Non-Negative Exponential Linear Unit\n",
    "    \"\"\"\n",
    "    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(input))\n",
    "#function to slice the returned list by the neural network\n",
    "def slice_parameter_vectors(parameter_vector):\n",
    "    \"\"\" Returns an unpacked list of paramter vectors.\n",
    "    \"\"\"\n",
    "    return [parameter_vector[:,i*components:(i+1\n",
    "    )*components] for i in range(no_parameters)]\n",
    "\n",
    "#function that calculates the loss \n",
    "def gnll_loss(y, parameter_vector):\n",
    "    \"\"\" Computes the mean negative log-likelihood loss of y given the mixture parameters.\n",
    "    \"\"\"\n",
    "    alpha,mu,sigma = slice_parameter_vectors(parameter_vector) # Unpack parameter vectors\n",
    "    #tf.print(sigma)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "           mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "           components_distribution=tfd.Normal(\n",
    "           loc=mu,       \n",
    "           scale=sigma))\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_likelihood =  gm.log_prob(tf.transpose(y)) # Evaluate log-probability of y \n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1) \n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'nnelu': Activation(nnelu)})\n",
    "\n",
    "def gnll_eval(y,alpha, mu, sigma):\n",
    "    \"\"\" Computes the mean negative log-likelihood loss of y given the mixture parameters.\n",
    "    \"\"\"\n",
    "    #print(alpha)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "        components_distribution=tfd.Normal(\n",
    "            loc=mu,       \n",
    "            scale=sigma))\n",
    "    log_likelihood = gm.log_prob(tf.transpose(y))\n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "\n",
    "def eval_mdn_model(x_test, y_test, mdn_model):\n",
    "    \"\"\" Evaluate the model to get the loss for the given x and y \n",
    "    \"\"\"\n",
    "    y_pred = mdn_model.predict(np.reshape(x_test,newshape=(len(x_test),-1)))\n",
    "    alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    mdn_nll = gnll_eval(y_test.astype(np.float32),alpha, mu, sigma).numpy()\n",
    "    return mdn_nll\n",
    "#reshapefunction\n",
    "def eval_mdn_model_mle(x_test,y_test):\n",
    "    \"\"\" Evaluate the model to get the loss by calculating the mle instead of fitting the model\n",
    "    \"\"\"\n",
    "    indices_1 = [i for i, x in enumerate(x_test) if x == 1]\n",
    "    #changing x to -1\n",
    "    indices_0 = [i for i, x in enumerate(x_test) if x == leftside]\n",
    "    mu_0=np.mean(y_test[indices_0])\n",
    "    mu_1=np.mean(y_test[indices_1])\n",
    "    sigma_0=np.std(y_test[indices_0])\n",
    "    sigma_1=np.std(y_test[indices_1])\n",
    "    y_mean=np.zeros((len(y_test),1))\n",
    "    y_mean[indices_1]=mu_1\n",
    "    y_mean[indices_0]=mu_0\n",
    "    y_std=np.zeros((len(y_test),1))\n",
    "    y_std[indices_1]=sigma_1\n",
    "    y_std[indices_0]=sigma_0\n",
    "    alpha=np.ones((len(y_mean),1))\n",
    "    return gnll_eval(y_test,alpha,y_mean,y_std).numpy()\n",
    "    \n",
    "def reshapevar(X):\n",
    "  \"\"\"\n",
    "  Function to reshape the vector for the input \n",
    "  \"\"\"\n",
    "  return np.reshape(X,newshape=(len(X),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the losses including mle \n",
    "def compute_loss(P,Q,mle=False):\n",
    "    #Compute the loss for the given pair\n",
    "    \n",
    "    if(mle==False):\n",
    "        opt = tf.optimizers.Adam(1e-2)\n",
    "        mdn_PQ = MDN_module()\n",
    "        mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "        mdn_PQ.fit(x=reshapevar(P), y=np.array(Q).T,epochs=100,  batch_size=64,verbose=0)\n",
    "        return eval_mdn_model(P,Q,mdn_PQ)\n",
    "    else:\n",
    "        return eval_mdn_model_mle(P,Q)\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return the predicted y \n",
    "def compute_loss_y_pred(P,Q,mle=False):\n",
    "    \"\"\" Computes the model prediction for a given pair\n",
    "    \"\"\"\n",
    "    if(mle==False):\n",
    "        opt = tf.optimizers.Adam(1e-2)\n",
    "        mdn_PQ = MDN_module()\n",
    "        mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "        mdn_PQ.fit(x=reshapevar(P), y=np.array(Q).T,epochs=100,  batch_size=64,verbose=0)\n",
    "        y_pred = mdn_PQ.predict(np.reshape(P,newshape=(len(P),-1)))\n",
    "        return y_pred[:,1]\n",
    "    else:\n",
    "        indices_1 = [i for i, x in enumerate(P) if x == 1]\n",
    "        indices_0 = [i for i, x in enumerate(P) if x == leftside]\n",
    "        mu_0=np.mean(Q[indices_0])\n",
    "        mu_1=np.mean(Q[indices_1])\n",
    "        #sigma_0=np.std(Q[indices_0])\n",
    "        #sigma_1=np.std(Q[indices_1])\n",
    "        y_mean=np.zeros((len(Q),1))\n",
    "        y_mean[indices_1]=mu_1\n",
    "        y_mean[indices_0]=mu_0\n",
    "        return y_mean.reshape((len(y_mean),))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used in L->A and L->B testing \n",
    "def shuffleBtimes(P,Q,B,mle=False):\n",
    "    \"\"\" Shuffle Q B times and compute the loss \n",
    "    \"\"\"\n",
    "    loss=[]\n",
    "    if(mle==False):\n",
    "        for i in range(0,B):\n",
    "          loss.append(compute_loss(P,np.random.permutation(Q)))\n",
    "    else: \n",
    "        for i in range(0,B):\n",
    "          loss.append(compute_loss(P,np.random.permutation(Q),True))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = np.random.binomial(1,0.5,1000)  \n",
    "# A=6*L + np.random.normal(0,1,1000)\n",
    "# B=np.empty([1000,])\n",
    "# B[L==1]=-3*(A[L==1]-6)*(A[L==1]-6)+np.random.normal(0,1,len(A[L==1]))+10\n",
    "# B[L==0]=3*(A[L==0])*(A[L==0])+np.random.normal(0,1,len(A[L==0]))-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.473929004669188\n"
     ]
    }
   ],
   "source": [
    "# A_test,y_pred_ones,y_pred_zeros=compute_third_testloss(L,A,B)\n",
    "# print(sum(abs(y_pred_zeros[:,1]-y_pred_ones[:,1]))/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(A,B)\n",
    "# plt.scatter(A_test,y_pred_ones[:,1])\n",
    "# plt.scatter(A_test,y_pred_zeros[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.08948091125488\n"
     ]
    }
   ],
   "source": [
    "# B_strat=stratifydata(L,B)\n",
    "# A_test,y_ones,y_zeros=compute_third_testloss(L,A,B_strat)\n",
    "# print(sum(abs(y_zeros[:,1]-y_ones[:,1]))/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(A,B_strat)\n",
    "# plt.scatter(A_test,y_ones[:,1])\n",
    "# plt.scatter(A_test,y_zeros[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to read the files \n",
    "linear=\"/data/users/cs20s037/CITNonLinear/LinearDifferentvalues/testing_writingvalues_Linear0to1.txt\"\n",
    "number_of_samples=121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fo=open(linear, \"r\")\n",
    "L=[]\n",
    "A=[]\n",
    "B=[]\n",
    "#fe=open(\"dataset_params.txt\",'w')\n",
    "for i in range(0,number_of_samples):\n",
    "    line=fo.readline()\n",
    "    #fe.write(line)\n",
    "    #line=line[1:-2] #remove double quotes \n",
    "    #param = [j for j in line.split()]\n",
    "    #print(param)\n",
    "    #chrname.append(param[1])\n",
    "    #g1.append(param[2])\n",
    "    #g2.append(param[3])\n",
    "    line=fo.readline()\n",
    "    l = [j for j in line.split()]\n",
    "    L.append([int(i) for i in l])\n",
    "    line=fo.readline()\n",
    "    a = [j for j in line.split()]\n",
    "    A.append([float(i) for i in a])\n",
    "    line=fo.readline()\n",
    "    b = [j for j in line.split()]\n",
    "    B.append([float(i) for i in b])\n",
    "dataset_linear = [i for i in zip(L,A,B)]\n",
    "fo.close()\n",
    "#fe.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pvalue(original,loss_list):\n",
    "    '''\n",
    "    calculate the p value \n",
    "    '''\n",
    "    return sum(i < original for i in loss_list)/len(loss_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to stratify the data\n",
    "def stratifydata(L,B):\n",
    "    indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "    #changin x==0 to x=-1\n",
    "    indices_0 = [i for i, x in enumerate(L) if x == leftside]\n",
    "    B_dist_temp=np.zeros(len(B))\n",
    "    mod_indices_1=random.sample(indices_1,len(indices_1))\n",
    "    for i in range(len(indices_1)):\n",
    "        B_dist_temp[indices_1[i]]=B[mod_indices_1[i]]\n",
    "\n",
    "    mod_indices_0=random.sample(indices_0,len(indices_0))\n",
    "    for i in range(len(indices_0)):\n",
    "        B_dist_temp[indices_0[i]]=B[mod_indices_0[i]]\n",
    "    return B_dist_temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_third_testloss(L,A,B,default=0):\n",
    "    if(default==1):  #SVM\n",
    "        regressor = SVR(kernel = 'rbf')\n",
    "        X=np.vstack((L,A)).T\n",
    "        regressor.fit(X,B)\n",
    "        L_ones=np.ones((L.shape))\n",
    "        L_minus=np.ones((L.shape))*leftside\n",
    "        X_zero=np.vstack((L_minus,A)).T\n",
    "        X_one=np.vstack((L_ones,A)).T\n",
    "        y_predict_zero=regressor.predict(X_zero)\n",
    "        y_predict_one=regressor.predict(X_one)\n",
    "        return [A,y_predict_one,y_predict_zero]\n",
    "    elif(default==0):\n",
    "    #mdn\n",
    "        opt = tf.optimizers.Adam(1e-2)\n",
    "        mdn_PQ = MDN_module()\n",
    "        mdn_PQ.compile(loss=gnll_loss, optimizer=opt)\n",
    "    ##changing epochs didnt make much difference\n",
    "        mdn_PQ.fit(x=np.concatenate([L.reshape(-1,1),A.reshape(-1,1)],axis=1),y=np.array(B).T,epochs=100,batch_size=64,verbose=0)\n",
    "    #mdn_PQ.fit(x=C, y=np.array(B).T,epochs=300,  batch_size=64)\n",
    "        #withoutL=mdn_PQ.fit(x=A, y=B.T,epochs=100,  batch_size=64,verbose=0)\n",
    "        L_ones=np.ones((L.shape))\n",
    "        #print(L_ones)\n",
    "        L_zeros=np.ones((L.shape))*leftside\n",
    "        #print(sum(abs(y_pred_zeros[:,1]-y_pred_ones[:,1]))/1000)\n",
    "        #print(min(A))\n",
    "        #print(max(A))\n",
    "        A_test=A\n",
    "        y_pred_ones= mdn_PQ.predict(np.concatenate([L_ones.reshape(-1,1),A_test.reshape(-1,1)],axis=1))\n",
    "        y_pred_zeros= mdn_PQ.predict(np.concatenate([L_zeros.reshape(-1,1),A_test.reshape(-1,1)],axis=1))\n",
    "        #y_pred = mdn_PQ.predict(A)\n",
    "        return [A_test,y_pred_ones[:,1],y_pred_zeros[:,1]]\n",
    "        #print(sum(y_pred_zeros[:,1]-y_pred_ones[:,1])/1000)\n",
    "        #alpha,mu,sigma = slice_parameter_vectors(y_pred)\n",
    "    elif(default==2):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_shape=(2,), activation='relu'))\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        X=np.vstack((L,A)).T\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "        model.fit(X, B, epochs=150, batch_size=10)\n",
    "        L_ones=np.ones((L.shape))\n",
    "        L_minus=np.ones((L.shape))*leftside\n",
    "        X_zero=np.vstack((L_minus,A)).T\n",
    "        X_one=np.vstack((L_ones,A)).T\n",
    "        y_predict_zero=model.predict(X_zero)\n",
    "        y_predict_one=model.predict(X_one)\n",
    "        y_predict_zero=y_predict_zero.reshape((len(y_predict_zero,)))\n",
    "        y_predict_one=y_predict_one.reshape((len(y_predict_one,)))\n",
    "        return [A,y_predict_one,y_predict_zero]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_B_n_times_diff(L,A,B,n):\n",
    "    loss=[]\n",
    "    indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "    #changing x to -1\n",
    "    indices_0 = [i for i, x in enumerate(L) if x == leftside]\n",
    "    mu_0=np.mean(A[indices_0])\n",
    "    mu_1=np.mean(A[indices_1])\n",
    "    sigma_0=np.std(A[indices_0])\n",
    "    sigma_1=np.std(A[indices_1])\n",
    "    p_L0=   np.count_nonzero(L==0)/len(L)\n",
    "    p_L1= np.count_nonzero(L==1)/len(L)\n",
    "    #indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "    #changin x==0 to x=-1\n",
    "    #indices_0 = [i for i, x in enumerate(L) if x == leftside]\n",
    "    for i in range(0,n):\n",
    "        B_dist_temp=np.zeros(len(B))\n",
    "        mod_indices_1=random.sample(indices_1,len(indices_1))\n",
    "        for i in range(len(indices_1)):\n",
    "            B_dist_temp[indices_1[i]]= B[mod_indices_1[i]]\n",
    "\n",
    "        mod_indices_0=random.sample(indices_0,len(indices_0))\n",
    "        for i in range(len(indices_0)):\n",
    "            B_dist_temp[indices_0[i]]= B[mod_indices_0[i]]\n",
    "        _,y_pred_ones,y_pred_zeros=compute_third_testloss(L,A,B_dist_temp,2)\n",
    "        \n",
    "        p_AgivenL0 = norm.pdf(A,mu_0,sigma_0)\n",
    "        p_AgivenL1 = norm.pdf(A,mu_1,sigma_1)\n",
    "        p_L0givenA = p_AgivenL0 * p_L0\n",
    "        p_L1givenA = p_AgivenL1 * p_L1\n",
    "    \n",
    "        diff=np.minimum(p_L0givenA,p_L1givenA)\n",
    "        loss.append(sum(np.multiply(abs(y_pred_zeros-y_pred_ones),diff))/1000)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=np.array([1,2,3])\n",
    "#b=np.array([4,5,0])\n",
    "#c=np.minimum(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e437977bf0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mshuffles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'j' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(j,j+n): \n",
    "    A=np.array(dataset_linear[i][1])\n",
    "    B=np.array(dataset_linear[i][2])\n",
    "    L=np.array(dataset_linear[i][0])\n",
    "    shuffles=100\n",
    "    A_shuffle=np.copy(A)\n",
    "    B_shuffle=np.copy(B)\n",
    "    #print(\"Original\",B_shuffle)\n",
    "    loss_list_LA=shuffleBtimes(L,A_shuffle,shuffles,True)\n",
    "    loss_list_LB=shuffleBtimes(L,B_shuffle,shuffles,True)\n",
    "    loss_list_Bresidual=stratify_B_n_times_diff(L,A_shuffle,B_shuffle,shuffles) #conditional independence test\n",
    "    true_LA=compute_loss(L,A,True)\n",
    "    true_LB=compute_loss(L,B,True)\n",
    "    #true_LBresidual=calculate_difference(L,A,B)\n",
    "    #loss_list_Bresidual,true_LBresidual=calculateLshuffle(L,A,B,shuffles)\n",
    "    indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "    #changing x to -1\n",
    "    indices_0 = [i for i, x in enumerate(L) if x == leftside]\n",
    "    mu_0=np.mean(A[indices_0])\n",
    "    mu_1=np.mean(A[indices_1])\n",
    "    sigma_0=np.std(A[indices_0])\n",
    "    sigma_1=np.std(A[indices_1])\n",
    "    p_L0=   np.count_nonzero(L==0)/len(L)\n",
    "    p_L1= np.count_nonzero(L==1)/len(L)\n",
    "    \n",
    "    _,y_pred_ones,y_pred_zeros=compute_third_testloss(L,A,B,2)\n",
    "    p_AgivenL0 = norm.pdf(A,mu_0,sigma_0)\n",
    "    p_AgivenL1 = norm.pdf(A,mu_1,sigma_1)\n",
    "    p_L0givenA = p_AgivenL0 * p_L0\n",
    "    p_L1givenA = p_AgivenL1 * p_L1\n",
    "\n",
    "    diff=np.minimum(p_L0givenA,p_L1givenA)\n",
    "    true_LBresidual=(sum(np.multiply(abs(y_pred_zeros-y_pred_ones),diff))/1000)\n",
    "    #true_LBresidual=sum(abs(y_pred_zeros-y_pred_ones)/1000)\n",
    "    LA_p=calculate_pvalue(true_LA,loss_list_LA)\n",
    "    LB_p=calculate_pvalue(true_LB,loss_list_LB)\n",
    "    AB_p=calculate_pvalue(true_LBresidual,loss_list_Bresidual)\n",
    "    \n",
    "    f.write(str(i)+\",\"+str(LA_p)+\",\"+str(LB_p)+\",\"+str(AB_p)+\"\\n\")\n",
    "    #f.write(str(indices[i])+\",\"+str(LA_p)+\",\"+str(LB_p)+\",\"+str(AB_p)+\"\\n\")\n",
    "    #pickle_items=[loss_list_LA,loss_list_LB,loss_list_Bresidual,true_LA,true_LB,true_LBresidual,LA_p,LB_p,AB_p]\n",
    "    #file_name=str(dataset_names[i])+\".pkl\"\n",
    "    #open_file = open(\"./DLresultspickle1000shuffle/\"+file_name, \"wb\")\n",
    "    #pickle.dump(pickle_items, open_file)\n",
    "    #open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.9765 - mean_squared_error: 0.9765\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 0s 970us/step - loss: 0.9663 - mean_squared_error: 0.9663\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 0s 941us/step - loss: 0.9653 - mean_squared_error: 0.9653\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.9634 - mean_squared_error: 0.9634\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 0s 998us/step - loss: 0.9635 - mean_squared_error: 0.9635\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 0s 958us/step - loss: 0.9631 - mean_squared_error: 0.9631\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 0s 704us/step - loss: 0.9624 - mean_squared_error: 0.9624\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 0s 630us/step - loss: 0.9622 - mean_squared_error: 0.9622\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 0s 619us/step - loss: 0.9621 - mean_squared_error: 0.9621\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 0s 627us/step - loss: 0.9635 - mean_squared_error: 0.9635\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 0s 625us/step - loss: 0.9606 - mean_squared_error: 0.9606\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.9607 - mean_squared_error: 0.9607\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 0s 623us/step - loss: 0.9615 - mean_squared_error: 0.9615\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 0s 645us/step - loss: 0.9590 - mean_squared_error: 0.9590\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 0s 632us/step - loss: 0.9607 - mean_squared_error: 0.9607\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 0s 623us/step - loss: 0.9596 - mean_squared_error: 0.9596\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 0s 627us/step - loss: 0.9598 - mean_squared_error: 0.9598\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 0s 608us/step - loss: 0.9581 - mean_squared_error: 0.9581\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 0s 619us/step - loss: 0.9581 - mean_squared_error: 0.9581\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 0s 661us/step - loss: 0.9572 - mean_squared_error: 0.9572\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 0s 621us/step - loss: 0.9567 - mean_squared_error: 0.9567\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 0s 602us/step - loss: 0.9570 - mean_squared_error: 0.9570\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 0s 610us/step - loss: 0.9560 - mean_squared_error: 0.9560\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 0s 625us/step - loss: 0.9560 - mean_squared_error: 0.9560\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.9557 - mean_squared_error: 0.9557\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 0s 740us/step - loss: 0.9562 - mean_squared_error: 0.9562\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.9568 - mean_squared_error: 0.9568\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 0s 641us/step - loss: 0.9569 - mean_squared_error: 0.9569\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 0s 699us/step - loss: 0.9569 - mean_squared_error: 0.9569\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.9558 - mean_squared_error: 0.9558\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 0s 650us/step - loss: 0.9571 - mean_squared_error: 0.9571\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 0s 726us/step - loss: 0.9553 - mean_squared_error: 0.9553\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 0s 619us/step - loss: 0.9560 - mean_squared_error: 0.9560\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 0s 635us/step - loss: 0.9553 - mean_squared_error: 0.9553\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 0s 636us/step - loss: 0.9565 - mean_squared_error: 0.9565\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.9552 - mean_squared_error: 0.9552\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 0s 613us/step - loss: 0.9550 - mean_squared_error: 0.9550\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.9550 - mean_squared_error: 0.9550\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 0s 654us/step - loss: 0.9549 - mean_squared_error: 0.9549\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 0s 690us/step - loss: 0.9553 - mean_squared_error: 0.9553\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 0s 666us/step - loss: 0.9552 - mean_squared_error: 0.9552\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 0s 646us/step - loss: 0.9550 - mean_squared_error: 0.9550\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 0s 701us/step - loss: 0.9549 - mean_squared_error: 0.9549\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 0s 726us/step - loss: 0.9552 - mean_squared_error: 0.9552\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9550 - mean_squared_error: 0.9550\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 0s 698us/step - loss: 0.9548 - mean_squared_error: 0.9548\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 0s 702us/step - loss: 0.9551 - mean_squared_error: 0.9551\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 0s 681us/step - loss: 0.9547 - mean_squared_error: 0.9547\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.9542 - mean_squared_error: 0.9542\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.9543 - mean_squared_error: 0.9543\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.9546 - mean_squared_error: 0.9546\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.9540 - mean_squared_error: 0.9540\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 0s 654us/step - loss: 0.9541 - mean_squared_error: 0.9541\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 0s 684us/step - loss: 0.9543 - mean_squared_error: 0.9543\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 0s 661us/step - loss: 0.9545 - mean_squared_error: 0.9545\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 0s 668us/step - loss: 0.9541 - mean_squared_error: 0.9541\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 0s 699us/step - loss: 0.9536 - mean_squared_error: 0.9536\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 0s 694us/step - loss: 0.9533 - mean_squared_error: 0.9533\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 0s 660us/step - loss: 0.9564 - mean_squared_error: 0.9564\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 0s 624us/step - loss: 0.9546 - mean_squared_error: 0.9546\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 0s 663us/step - loss: 0.9541 - mean_squared_error: 0.9541\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 0s 697us/step - loss: 0.9534 - mean_squared_error: 0.9534\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 0s 709us/step - loss: 0.9536 - mean_squared_error: 0.9536\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 0s 648us/step - loss: 0.9545 - mean_squared_error: 0.9545\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 0s 679us/step - loss: 0.9541 - mean_squared_error: 0.9541\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 0s 683us/step - loss: 0.9551 - mean_squared_error: 0.9551\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 0s 742us/step - loss: 0.9535 - mean_squared_error: 0.9535\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9542 - mean_squared_error: 0.9542\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 0s 660us/step - loss: 0.9535 - mean_squared_error: 0.9535\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.9530 - mean_squared_error: 0.9530\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 0s 622us/step - loss: 0.9537 - mean_squared_error: 0.9537\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 0s 604us/step - loss: 0.9529 - mean_squared_error: 0.9529\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 0s 662us/step - loss: 0.9539 - mean_squared_error: 0.9539\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 0s 720us/step - loss: 0.9535 - mean_squared_error: 0.9535\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 0s 694us/step - loss: 0.9525 - mean_squared_error: 0.9525\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.9531 - mean_squared_error: 0.9531\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.9544 - mean_squared_error: 0.9544\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 0s 670us/step - loss: 0.9533 - mean_squared_error: 0.9533\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 0s 674us/step - loss: 0.9529 - mean_squared_error: 0.9529\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 0s 721us/step - loss: 0.9530 - mean_squared_error: 0.9530\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 0s 693us/step - loss: 0.9529 - mean_squared_error: 0.9529\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 0s 697us/step - loss: 0.9524 - mean_squared_error: 0.9524\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.9536 - mean_squared_error: 0.9536\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 0s 673us/step - loss: 0.9550 - mean_squared_error: 0.9550\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 0s 737us/step - loss: 0.9540 - mean_squared_error: 0.9540\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 0s 668us/step - loss: 0.9534 - mean_squared_error: 0.9534\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 0s 690us/step - loss: 0.9533 - mean_squared_error: 0.9533\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 0s 720us/step - loss: 0.9534 - mean_squared_error: 0.9534\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 0s 711us/step - loss: 0.9530 - mean_squared_error: 0.9530\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 0s 718us/step - loss: 0.9526 - mean_squared_error: 0.9526\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 0s 728us/step - loss: 0.9528 - mean_squared_error: 0.9528\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.9525 - mean_squared_error: 0.9525\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 0s 691us/step - loss: 0.9525 - mean_squared_error: 0.9525\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 0s 694us/step - loss: 0.9532 - mean_squared_error: 0.9532\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 0s 685us/step - loss: 0.9519 - mean_squared_error: 0.9519\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 0s 676us/step - loss: 0.9529 - mean_squared_error: 0.9529\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 0s 751us/step - loss: 0.9545 - mean_squared_error: 0.9545\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 0s 841us/step - loss: 0.9535 - mean_squared_error: 0.9535\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 0s 670us/step - loss: 0.9528 - mean_squared_error: 0.9528\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 0s 698us/step - loss: 0.9518 - mean_squared_error: 0.9518\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 0s 708us/step - loss: 0.9531 - mean_squared_error: 0.9531\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 0s 700us/step - loss: 0.9527 - mean_squared_error: 0.9527\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 0s 687us/step - loss: 0.9528 - mean_squared_error: 0.9528\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 0s 696us/step - loss: 0.9535 - mean_squared_error: 0.9535\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 0s 676us/step - loss: 0.9529 - mean_squared_error: 0.9529\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 0s 715us/step - loss: 0.9530 - mean_squared_error: 0.9530\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9526 - mean_squared_error: 0.9526\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 0s 718us/step - loss: 0.9519 - mean_squared_error: 0.9519\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 0s 700us/step - loss: 0.9520 - mean_squared_error: 0.9520\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.9521 - mean_squared_error: 0.9521\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 0s 708us/step - loss: 0.9523 - mean_squared_error: 0.9523\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 0s 731us/step - loss: 0.9518 - mean_squared_error: 0.9518\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 0s 751us/step - loss: 0.9517 - mean_squared_error: 0.9517\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 0s 711us/step - loss: 0.9527 - mean_squared_error: 0.9527\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9521 - mean_squared_error: 0.9521\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 0s 714us/step - loss: 0.9529 - mean_squared_error: 0.9529\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 0s 690us/step - loss: 0.9514 - mean_squared_error: 0.9514\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 0s 697us/step - loss: 0.9525 - mean_squared_error: 0.9525\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.9519 - mean_squared_error: 0.9519\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 0s 723us/step - loss: 0.9515 - mean_squared_error: 0.9515\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 0s 698us/step - loss: 0.9517 - mean_squared_error: 0.9517\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 0s 712us/step - loss: 0.9522 - mean_squared_error: 0.9522\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 0s 721us/step - loss: 0.9518 - mean_squared_error: 0.9518\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.9510 - mean_squared_error: 0.9510\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 0s 760us/step - loss: 0.9515 - mean_squared_error: 0.9515\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 0s 717us/step - loss: 0.9510 - mean_squared_error: 0.9510\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 0s 722us/step - loss: 0.9510 - mean_squared_error: 0.9510\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 0s 707us/step - loss: 0.9515 - mean_squared_error: 0.9515\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.9514 - mean_squared_error: 0.9514\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 0s 719us/step - loss: 0.9513 - mean_squared_error: 0.9513\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 0s 733us/step - loss: 0.9513 - mean_squared_error: 0.9513\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 0s 660us/step - loss: 0.9532 - mean_squared_error: 0.9532\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 0s 671us/step - loss: 0.9512 - mean_squared_error: 0.9512\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 0s 628us/step - loss: 0.9518 - mean_squared_error: 0.9518\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 0s 628us/step - loss: 0.9516 - mean_squared_error: 0.9516\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 0s 650us/step - loss: 0.9519 - mean_squared_error: 0.9519\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 0s 627us/step - loss: 0.9512 - mean_squared_error: 0.9512\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 0s 632us/step - loss: 0.9523 - mean_squared_error: 0.9523\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 0s 620us/step - loss: 0.9512 - mean_squared_error: 0.9512\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 0s 568us/step - loss: 0.9516 - mean_squared_error: 0.9516\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 0s 594us/step - loss: 0.9515 - mean_squared_error: 0.9515\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 0s 674us/step - loss: 0.9519 - mean_squared_error: 0.9519\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 0s 636us/step - loss: 0.9520 - mean_squared_error: 0.9520\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 0s 655us/step - loss: 0.9511 - mean_squared_error: 0.9511\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 0s 624us/step - loss: 0.9507 - mean_squared_error: 0.9507\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 0s 646us/step - loss: 0.9519 - mean_squared_error: 0.9519\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 0s 630us/step - loss: 0.9531 - mean_squared_error: 0.9531\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.9523 - mean_squared_error: 0.9523\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 0s 629us/step - loss: 0.9513 - mean_squared_error: 0.9513\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 0s 624us/step - loss: 0.9507 - mean_squared_error: 0.9507\n",
      "Epoch 1/150\n",
      "100/100 [==============================] - 0s 778us/step - loss: 0.9795 - mean_squared_error: 0.9795\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 0s 646us/step - loss: 0.9635 - mean_squared_error: 0.9635\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 0s 705us/step - loss: 0.9603 - mean_squared_error: 0.9603\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9566 - mean_squared_error: 0.9566\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 0s 729us/step - loss: 0.9560 - mean_squared_error: 0.9560\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 0s 681us/step - loss: 0.9546 - mean_squared_error: 0.9546\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 0s 700us/step - loss: 0.9540 - mean_squared_error: 0.9540\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 0s 656us/step - loss: 0.9527 - mean_squared_error: 0.9527\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 0s 661us/step - loss: 0.9520 - mean_squared_error: 0.9520\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 0s 646us/step - loss: 0.9516 - mean_squared_error: 0.9516\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 0s 659us/step - loss: 0.9512 - mean_squared_error: 0.9512\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 0s 708us/step - loss: 0.9514 - mean_squared_error: 0.9514\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 0s 597us/step - loss: 0.9507 - mean_squared_error: 0.9507\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 0s 563us/step - loss: 0.9511 - mean_squared_error: 0.9511\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 0s 579us/step - loss: 0.9495 - mean_squared_error: 0.9495\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 0s 569us/step - loss: 0.9491 - mean_squared_error: 0.9491\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 0s 579us/step - loss: 0.9493 - mean_squared_error: 0.9493\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 0s 557us/step - loss: 0.9504 - mean_squared_error: 0.9504\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 0s 546us/step - loss: 0.9504 - mean_squared_error: 0.9504\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 0s 545us/step - loss: 0.9494 - mean_squared_error: 0.9494\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 0s 553us/step - loss: 0.9498 - mean_squared_error: 0.9498\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 0s 555us/step - loss: 0.9498 - mean_squared_error: 0.9498\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 0s 632us/step - loss: 0.9507 - mean_squared_error: 0.9507\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 0s 580us/step - loss: 0.9484 - mean_squared_error: 0.9484\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 0s 556us/step - loss: 0.9479 - mean_squared_error: 0.9479\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 0s 565us/step - loss: 0.9485 - mean_squared_error: 0.9485\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 0s 604us/step - loss: 0.9490 - mean_squared_error: 0.9490\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 0s 563us/step - loss: 0.9481 - mean_squared_error: 0.9481\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 0s 552us/step - loss: 0.9486 - mean_squared_error: 0.9486\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 0s 577us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 0s 564us/step - loss: 0.9477 - mean_squared_error: 0.9477\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 0s 604us/step - loss: 0.9489 - mean_squared_error: 0.9489\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 0s 602us/step - loss: 0.9502 - mean_squared_error: 0.9502\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 0s 554us/step - loss: 0.9496 - mean_squared_error: 0.9496\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 0s 545us/step - loss: 0.9474 - mean_squared_error: 0.9474\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 0s 540us/step - loss: 0.9481 - mean_squared_error: 0.9481\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 0s 558us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 0s 563us/step - loss: 0.9473 - mean_squared_error: 0.9473\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 0s 536us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 0s 534us/step - loss: 0.9486 - mean_squared_error: 0.9486\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 0s 578us/step - loss: 0.9483 - mean_squared_error: 0.9483\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 0s 573us/step - loss: 0.9488 - mean_squared_error: 0.9488\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 0s 554us/step - loss: 0.9483 - mean_squared_error: 0.9483\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 0s 633us/step - loss: 0.9486 - mean_squared_error: 0.9486\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 0s 593us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 0s 551us/step - loss: 0.9474 - mean_squared_error: 0.9474\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 0s 577us/step - loss: 0.9471 - mean_squared_error: 0.9471\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 0s 555us/step - loss: 0.9473 - mean_squared_error: 0.9473\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 0s 538us/step - loss: 0.9491 - mean_squared_error: 0.9491\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 0s 569us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 0s 542us/step - loss: 0.9469 - mean_squared_error: 0.9469\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 0s 564us/step - loss: 0.9479 - mean_squared_error: 0.9479\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 0s 585us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 0s 575us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 0s 570us/step - loss: 0.9484 - mean_squared_error: 0.9484\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 0s 552us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 0s 547us/step - loss: 0.9474 - mean_squared_error: 0.9474\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 0s 569us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 0s 585us/step - loss: 0.9480 - mean_squared_error: 0.9480\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 0s 569us/step - loss: 0.9476 - mean_squared_error: 0.9476\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 0s 551us/step - loss: 0.9467 - mean_squared_error: 0.9467\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 0s 535us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 0s 535us/step - loss: 0.9476 - mean_squared_error: 0.9476\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 0s 537us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 0s 571us/step - loss: 0.9480 - mean_squared_error: 0.9480\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 0s 554us/step - loss: 0.9471 - mean_squared_error: 0.9471\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 0s 576us/step - loss: 0.9472 - mean_squared_error: 0.9472\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 0s 595us/step - loss: 0.9477 - mean_squared_error: 0.9477\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 0s 561us/step - loss: 0.9475 - mean_squared_error: 0.9475\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 0s 538us/step - loss: 0.9467 - mean_squared_error: 0.9467\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 0s 548us/step - loss: 0.9471 - mean_squared_error: 0.9471\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 0s 584us/step - loss: 0.9464 - mean_squared_error: 0.9464\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 0s 587us/step - loss: 0.9471 - mean_squared_error: 0.9471\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 0s 556us/step - loss: 0.9464 - mean_squared_error: 0.9464\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 0s 541us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 0s 529us/step - loss: 0.9464 - mean_squared_error: 0.9464\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 0s 530us/step - loss: 0.9470 - mean_squared_error: 0.9470\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 0s 533us/step - loss: 0.9464 - mean_squared_error: 0.9464\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 0s 561us/step - loss: 0.9473 - mean_squared_error: 0.9473\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 0s 549us/step - loss: 0.9462 - mean_squared_error: 0.9462\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 0s 558us/step - loss: 0.9480 - mean_squared_error: 0.9480\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 0s 559us/step - loss: 0.9464 - mean_squared_error: 0.9464\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 0s 568us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 0s 562us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 0s 550us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 0s 584us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 0s 665us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 0s 650us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 0s 650us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 0s 655us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 0s 655us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 0s 656us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.9468 - mean_squared_error: 0.9468\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.9453 - mean_squared_error: 0.9453\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 0s 643us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 0s 611us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 0s 571us/step - loss: 0.9462 - mean_squared_error: 0.9462\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 0s 577us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 0s 583us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 0s 558us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 0s 548us/step - loss: 0.9462 - mean_squared_error: 0.9462\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 0s 555us/step - loss: 0.9448 - mean_squared_error: 0.9448\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 0s 589us/step - loss: 0.9458 - mean_squared_error: 0.9458\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 0s 565us/step - loss: 0.9455 - mean_squared_error: 0.9455\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 0s 567us/step - loss: 0.9455 - mean_squared_error: 0.9455\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 0s 570us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 0s 570us/step - loss: 0.9470 - mean_squared_error: 0.9470\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 0s 570us/step - loss: 0.9475 - mean_squared_error: 0.9475\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 0s 548us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 0s 587us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 0s 603us/step - loss: 0.9456 - mean_squared_error: 0.9456\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 0s 575us/step - loss: 0.9467 - mean_squared_error: 0.9467\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 0s 585us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 0s 581us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 0s 565us/step - loss: 0.9455 - mean_squared_error: 0.9455\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 0s 568us/step - loss: 0.9458 - mean_squared_error: 0.9458\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 0s 552us/step - loss: 0.9454 - mean_squared_error: 0.9454\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 0s 582us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 0s 564us/step - loss: 0.9470 - mean_squared_error: 0.9470\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 0s 559us/step - loss: 0.9449 - mean_squared_error: 0.9449\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 0s 552us/step - loss: 0.9456 - mean_squared_error: 0.9456\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 0s 571us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 0s 541us/step - loss: 0.9455 - mean_squared_error: 0.9455\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 0s 570us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 0s 569us/step - loss: 0.9458 - mean_squared_error: 0.9458\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 0s 561us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 0s 558us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 0s 543us/step - loss: 0.9456 - mean_squared_error: 0.9456\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 0s 579us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 0s 560us/step - loss: 0.9452 - mean_squared_error: 0.9452\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 0s 541us/step - loss: 0.9453 - mean_squared_error: 0.9453\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 0s 583us/step - loss: 0.9474 - mean_squared_error: 0.9474\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 0s 562us/step - loss: 0.9456 - mean_squared_error: 0.9456\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 0s 560us/step - loss: 0.9448 - mean_squared_error: 0.9448\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 0s 552us/step - loss: 0.9453 - mean_squared_error: 0.9453\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 0s 583us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 0s 563us/step - loss: 0.9451 - mean_squared_error: 0.9451\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 0s 569us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 0s 579us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 0s 563us/step - loss: 0.9447 - mean_squared_error: 0.9447\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 0s 562us/step - loss: 0.9445 - mean_squared_error: 0.9445\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 0s 570us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 0s 540us/step - loss: 0.9464 - mean_squared_error: 0.9464\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 0s 558us/step - loss: 0.9450 - mean_squared_error: 0.9450\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 0s 525us/step - loss: 0.9455 - mean_squared_error: 0.9455\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 0s 524us/step - loss: 0.9461 - mean_squared_error: 0.9461\n"
     ]
    }
   ],
   "source": [
    "# A=np.array(dataset_linear[i][1])\n",
    "# B=np.array(dataset_linear[i][2])\n",
    "# L=np.array(dataset_linear[i][0])\n",
    "# shuffles=1\n",
    "# A_shuffle=np.copy(A)\n",
    "# B_shuffle=np.copy(B)\n",
    "# #print(\"Original\",B_shuffle)\n",
    "# loss_list_LA=shuffleBtimes(L,A_shuffle,shuffles,True)\n",
    "# loss_list_LB=shuffleBtimes(L,B_shuffle,shuffles,True)\n",
    "# loss_list_Bresidual=stratify_B_n_times_diff(L,A_shuffle,B_shuffle,shuffles) #conditional independence test\n",
    "# true_LA=compute_loss(L,A,True)\n",
    "# true_LB=compute_loss(L,B,True)\n",
    "# #true_LBresidual=calculate_difference(L,A,B)\n",
    "# #loss_list_Bresidual,true_LBresidual=calculateLshuffle(L,A,B,shuffles)\n",
    "# indices_1 = [i for i, x in enumerate(L) if x == 1]\n",
    "# #changing x to -1\n",
    "# indices_0 = [i for i, x in enumerate(L) if x == leftside]\n",
    "# mu_0=np.mean(A[indices_0])\n",
    "# mu_1=np.mean(A[indices_1])\n",
    "# sigma_0=np.std(A[indices_0])\n",
    "# sigma_1=np.std(A[indices_1])\n",
    "# p_L0=   np.count_nonzero(L==0)/len(L)\n",
    "# p_L1= np.count_nonzero(L==1)/len(L)\n",
    "\n",
    "# _,y_pred_ones,y_pred_zeros=compute_third_testloss(L,A,B,2)\n",
    "# p_AgivenL0 = norm.pdf(A,mu_0,sigma_0)\n",
    "# p_AgivenL1 = norm.pdf(A,mu_1,sigma_1)\n",
    "# p_L0givenA = p_AgivenL0 * p_L0\n",
    "# p_L1givenA = p_AgivenL1 * p_L1\n",
    "\n",
    "# diff=np.minimum(p_L0givenA,p_L1givenA)\n",
    "# true_LBresidual=(sum(np.multiply(abs(y_pred_zeros-y_pred_ones),diff))/1000)\n",
    "# #true_LBresidual=sum(abs(y_pred_zeros-y_pred_ones)/1000)\n",
    "# LA_p=calculate_pvalue(true_LA,loss_list_LA)\n",
    "# LB_p=calculate_pvalue(true_LB,loss_list_LB)\n",
    "# AB_p=calculate_pvalue(true_LBresidual,loss_list_Bresidual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AB_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor = SVR(kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A=np.array(dataset_linear[i][1])\n",
    "# B=np.array(dataset_linear[i][2])\n",
    "# L=np.array(dataset_linear[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'A vs B')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.scatter(A,B,c=L)\n",
    "#  plt.title(\"A vs B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Dense(12, input_shape=(2,), activation='relu'))\n",
    "# model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.9880 - mean_squared_error: 0.9880\n",
      "Epoch 2/150\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.9588 - mean_squared_error: 0.9588\n",
      "Epoch 3/150\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.9540 - mean_squared_error: 0.9540\n",
      "Epoch 4/150\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.9544 - mean_squared_error: 0.9544\n",
      "Epoch 5/150\n",
      "100/100 [==============================] - 0s 895us/step - loss: 0.9522 - mean_squared_error: 0.9522\n",
      "Epoch 6/150\n",
      "100/100 [==============================] - 0s 790us/step - loss: 0.9512 - mean_squared_error: 0.9512\n",
      "Epoch 7/150\n",
      "100/100 [==============================] - 0s 718us/step - loss: 0.9522 - mean_squared_error: 0.9522\n",
      "Epoch 8/150\n",
      "100/100 [==============================] - 0s 688us/step - loss: 0.9524 - mean_squared_error: 0.9524\n",
      "Epoch 9/150\n",
      "100/100 [==============================] - 0s 688us/step - loss: 0.9525 - mean_squared_error: 0.9525\n",
      "Epoch 10/150\n",
      "100/100 [==============================] - 0s 671us/step - loss: 0.9500 - mean_squared_error: 0.9500\n",
      "Epoch 11/150\n",
      "100/100 [==============================] - 0s 672us/step - loss: 0.9504 - mean_squared_error: 0.9504\n",
      "Epoch 12/150\n",
      "100/100 [==============================] - 0s 681us/step - loss: 0.9507 - mean_squared_error: 0.9507\n",
      "Epoch 13/150\n",
      "100/100 [==============================] - 0s 640us/step - loss: 0.9505 - mean_squared_error: 0.9505\n",
      "Epoch 14/150\n",
      "100/100 [==============================] - 0s 613us/step - loss: 0.9501 - mean_squared_error: 0.9501\n",
      "Epoch 15/150\n",
      "100/100 [==============================] - 0s 622us/step - loss: 0.9496 - mean_squared_error: 0.9496\n",
      "Epoch 16/150\n",
      "100/100 [==============================] - 0s 614us/step - loss: 0.9513 - mean_squared_error: 0.9513\n",
      "Epoch 17/150\n",
      "100/100 [==============================] - 0s 656us/step - loss: 0.9497 - mean_squared_error: 0.9497\n",
      "Epoch 18/150\n",
      "100/100 [==============================] - 0s 645us/step - loss: 0.9512 - mean_squared_error: 0.9512\n",
      "Epoch 19/150\n",
      "100/100 [==============================] - 0s 626us/step - loss: 0.9515 - mean_squared_error: 0.9515\n",
      "Epoch 20/150\n",
      "100/100 [==============================] - 0s 644us/step - loss: 0.9487 - mean_squared_error: 0.9487\n",
      "Epoch 21/150\n",
      "100/100 [==============================] - 0s 618us/step - loss: 0.9512 - mean_squared_error: 0.9512\n",
      "Epoch 22/150\n",
      "100/100 [==============================] - 0s 619us/step - loss: 0.9502 - mean_squared_error: 0.9502\n",
      "Epoch 23/150\n",
      "100/100 [==============================] - 0s 621us/step - loss: 0.9517 - mean_squared_error: 0.9517\n",
      "Epoch 24/150\n",
      "100/100 [==============================] - 0s 670us/step - loss: 0.9488 - mean_squared_error: 0.9488\n",
      "Epoch 25/150\n",
      "100/100 [==============================] - 0s 719us/step - loss: 0.9500 - mean_squared_error: 0.9500\n",
      "Epoch 26/150\n",
      "100/100 [==============================] - 0s 695us/step - loss: 0.9509 - mean_squared_error: 0.9509\n",
      "Epoch 27/150\n",
      "100/100 [==============================] - 0s 769us/step - loss: 0.9485 - mean_squared_error: 0.9485\n",
      "Epoch 28/150\n",
      "100/100 [==============================] - 0s 739us/step - loss: 0.9498 - mean_squared_error: 0.9498\n",
      "Epoch 29/150\n",
      "100/100 [==============================] - 0s 673us/step - loss: 0.9508 - mean_squared_error: 0.9508\n",
      "Epoch 30/150\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9492 - mean_squared_error: 0.9492\n",
      "Epoch 31/150\n",
      "100/100 [==============================] - 0s 656us/step - loss: 0.9495 - mean_squared_error: 0.9495\n",
      "Epoch 32/150\n",
      "100/100 [==============================] - 0s 735us/step - loss: 0.9479 - mean_squared_error: 0.9479\n",
      "Epoch 33/150\n",
      "100/100 [==============================] - 0s 706us/step - loss: 0.9493 - mean_squared_error: 0.9493\n",
      "Epoch 34/150\n",
      "100/100 [==============================] - 0s 712us/step - loss: 0.9488 - mean_squared_error: 0.9488\n",
      "Epoch 35/150\n",
      "100/100 [==============================] - 0s 736us/step - loss: 0.9497 - mean_squared_error: 0.9497\n",
      "Epoch 36/150\n",
      "100/100 [==============================] - 0s 693us/step - loss: 0.9488 - mean_squared_error: 0.9488\n",
      "Epoch 37/150\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 38/150\n",
      "100/100 [==============================] - 0s 741us/step - loss: 0.9489 - mean_squared_error: 0.9489\n",
      "Epoch 39/150\n",
      "100/100 [==============================] - 0s 738us/step - loss: 0.9481 - mean_squared_error: 0.9481\n",
      "Epoch 40/150\n",
      "100/100 [==============================] - 0s 648us/step - loss: 0.9497 - mean_squared_error: 0.9497\n",
      "Epoch 41/150\n",
      "100/100 [==============================] - 0s 671us/step - loss: 0.9494 - mean_squared_error: 0.9494\n",
      "Epoch 42/150\n",
      "100/100 [==============================] - 0s 745us/step - loss: 0.9476 - mean_squared_error: 0.9476\n",
      "Epoch 43/150\n",
      "100/100 [==============================] - 0s 652us/step - loss: 0.9484 - mean_squared_error: 0.9484\n",
      "Epoch 44/150\n",
      "100/100 [==============================] - 0s 650us/step - loss: 0.9469 - mean_squared_error: 0.9469\n",
      "Epoch 45/150\n",
      "100/100 [==============================] - 0s 585us/step - loss: 0.9482 - mean_squared_error: 0.9482\n",
      "Epoch 46/150\n",
      "100/100 [==============================] - 0s 544us/step - loss: 0.9475 - mean_squared_error: 0.9475\n",
      "Epoch 47/150\n",
      "100/100 [==============================] - 0s 551us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 48/150\n",
      "100/100 [==============================] - 0s 546us/step - loss: 0.9479 - mean_squared_error: 0.9479\n",
      "Epoch 49/150\n",
      "100/100 [==============================] - 0s 574us/step - loss: 0.9486 - mean_squared_error: 0.9486\n",
      "Epoch 50/150\n",
      "100/100 [==============================] - 0s 582us/step - loss: 0.9482 - mean_squared_error: 0.9482\n",
      "Epoch 51/150\n",
      "100/100 [==============================] - 0s 553us/step - loss: 0.9475 - mean_squared_error: 0.9475\n",
      "Epoch 52/150\n",
      "100/100 [==============================] - 0s 536us/step - loss: 0.9467 - mean_squared_error: 0.9467\n",
      "Epoch 53/150\n",
      "100/100 [==============================] - 0s 577us/step - loss: 0.9481 - mean_squared_error: 0.9481\n",
      "Epoch 54/150\n",
      "100/100 [==============================] - 0s 594us/step - loss: 0.9471 - mean_squared_error: 0.9471\n",
      "Epoch 55/150\n",
      "100/100 [==============================] - 0s 573us/step - loss: 0.9467 - mean_squared_error: 0.9467\n",
      "Epoch 56/150\n",
      "100/100 [==============================] - 0s 577us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 57/150\n",
      "100/100 [==============================] - 0s 541us/step - loss: 0.9468 - mean_squared_error: 0.9468\n",
      "Epoch 58/150\n",
      "100/100 [==============================] - 0s 614us/step - loss: 0.9479 - mean_squared_error: 0.9479\n",
      "Epoch 59/150\n",
      "100/100 [==============================] - 0s 718us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 60/150\n",
      "100/100 [==============================] - 0s 616us/step - loss: 0.9477 - mean_squared_error: 0.9477\n",
      "Epoch 61/150\n",
      "100/100 [==============================] - 0s 577us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 62/150\n",
      "100/100 [==============================] - 0s 589us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 63/150\n",
      "100/100 [==============================] - 0s 689us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 64/150\n",
      "100/100 [==============================] - 0s 669us/step - loss: 0.9462 - mean_squared_error: 0.9462\n",
      "Epoch 65/150\n",
      "100/100 [==============================] - 0s 624us/step - loss: 0.9469 - mean_squared_error: 0.9469\n",
      "Epoch 66/150\n",
      "100/100 [==============================] - 0s 634us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 67/150\n",
      "100/100 [==============================] - 0s 630us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 68/150\n",
      "100/100 [==============================] - 0s 629us/step - loss: 0.9480 - mean_squared_error: 0.9480\n",
      "Epoch 69/150\n",
      "100/100 [==============================] - 0s 622us/step - loss: 0.9452 - mean_squared_error: 0.9452\n",
      "Epoch 70/150\n",
      "100/100 [==============================] - 0s 642us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 71/150\n",
      "100/100 [==============================] - 0s 692us/step - loss: 0.9454 - mean_squared_error: 0.9454\n",
      "Epoch 72/150\n",
      "100/100 [==============================] - 0s 628us/step - loss: 0.9473 - mean_squared_error: 0.9473\n",
      "Epoch 73/150\n",
      "100/100 [==============================] - 0s 680us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 74/150\n",
      "100/100 [==============================] - 0s 635us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 75/150\n",
      "100/100 [==============================] - 0s 706us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 76/150\n",
      "100/100 [==============================] - 0s 654us/step - loss: 0.9478 - mean_squared_error: 0.9478\n",
      "Epoch 77/150\n",
      "100/100 [==============================] - 0s 628us/step - loss: 0.9475 - mean_squared_error: 0.9475\n",
      "Epoch 78/150\n",
      "100/100 [==============================] - 0s 667us/step - loss: 0.9467 - mean_squared_error: 0.9467\n",
      "Epoch 79/150\n",
      "100/100 [==============================] - 0s 656us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 80/150\n",
      "100/100 [==============================] - 0s 641us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 81/150\n",
      "100/100 [==============================] - 0s 668us/step - loss: 0.9492 - mean_squared_error: 0.9492\n",
      "Epoch 82/150\n",
      "100/100 [==============================] - 0s 643us/step - loss: 0.9492 - mean_squared_error: 0.9492\n",
      "Epoch 83/150\n",
      "100/100 [==============================] - 0s 629us/step - loss: 0.9475 - mean_squared_error: 0.9475\n",
      "Epoch 84/150\n",
      "100/100 [==============================] - 0s 660us/step - loss: 0.9459 - mean_squared_error: 0.9459\n",
      "Epoch 85/150\n",
      "100/100 [==============================] - 0s 612us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 86/150\n",
      "100/100 [==============================] - 0s 569us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 87/150\n",
      "100/100 [==============================] - 0s 552us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 88/150\n",
      "100/100 [==============================] - 0s 546us/step - loss: 0.9474 - mean_squared_error: 0.9474\n",
      "Epoch 89/150\n",
      "100/100 [==============================] - 0s 556us/step - loss: 0.9461 - mean_squared_error: 0.9461\n",
      "Epoch 90/150\n",
      "100/100 [==============================] - 0s 640us/step - loss: 0.9450 - mean_squared_error: 0.9450\n",
      "Epoch 91/150\n",
      "100/100 [==============================] - 0s 626us/step - loss: 0.9455 - mean_squared_error: 0.9455\n",
      "Epoch 92/150\n",
      "100/100 [==============================] - 0s 628us/step - loss: 0.9467 - mean_squared_error: 0.9467\n",
      "Epoch 93/150\n",
      "100/100 [==============================] - 0s 674us/step - loss: 0.9454 - mean_squared_error: 0.9454\n",
      "Epoch 94/150\n",
      "100/100 [==============================] - 0s 678us/step - loss: 0.9450 - mean_squared_error: 0.9450\n",
      "Epoch 95/150\n",
      "100/100 [==============================] - 0s 621us/step - loss: 0.9447 - mean_squared_error: 0.9447\n",
      "Epoch 96/150\n",
      "100/100 [==============================] - 0s 698us/step - loss: 0.9462 - mean_squared_error: 0.9462\n",
      "Epoch 97/150\n",
      "100/100 [==============================] - 0s 618us/step - loss: 0.9473 - mean_squared_error: 0.9473\n",
      "Epoch 98/150\n",
      "100/100 [==============================] - 0s 620us/step - loss: 0.9450 - mean_squared_error: 0.9450\n",
      "Epoch 99/150\n",
      "100/100 [==============================] - 0s 636us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 100/150\n",
      "100/100 [==============================] - 0s 643us/step - loss: 0.9462 - mean_squared_error: 0.9462\n",
      "Epoch 101/150\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.9451 - mean_squared_error: 0.9451\n",
      "Epoch 102/150\n",
      "100/100 [==============================] - 0s 688us/step - loss: 0.9446 - mean_squared_error: 0.9446\n",
      "Epoch 103/150\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.9442 - mean_squared_error: 0.9442\n",
      "Epoch 104/150\n",
      "100/100 [==============================] - 0s 658us/step - loss: 0.9444 - mean_squared_error: 0.9444\n",
      "Epoch 105/150\n",
      "100/100 [==============================] - 0s 696us/step - loss: 0.9442 - mean_squared_error: 0.9442\n",
      "Epoch 106/150\n",
      "100/100 [==============================] - 0s 714us/step - loss: 0.9436 - mean_squared_error: 0.9436\n",
      "Epoch 107/150\n",
      "100/100 [==============================] - 0s 709us/step - loss: 0.9454 - mean_squared_error: 0.9454\n",
      "Epoch 108/150\n",
      "100/100 [==============================] - 0s 685us/step - loss: 0.9447 - mean_squared_error: 0.9447\n",
      "Epoch 109/150\n",
      "100/100 [==============================] - 0s 701us/step - loss: 0.9448 - mean_squared_error: 0.9448\n",
      "Epoch 110/150\n",
      "100/100 [==============================] - 0s 701us/step - loss: 0.9481 - mean_squared_error: 0.9481\n",
      "Epoch 111/150\n",
      "100/100 [==============================] - 0s 674us/step - loss: 0.9445 - mean_squared_error: 0.9445\n",
      "Epoch 112/150\n",
      "100/100 [==============================] - 0s 647us/step - loss: 0.9468 - mean_squared_error: 0.9468\n",
      "Epoch 113/150\n",
      "100/100 [==============================] - 0s 637us/step - loss: 0.9463 - mean_squared_error: 0.9463\n",
      "Epoch 114/150\n",
      "100/100 [==============================] - 0s 634us/step - loss: 0.9447 - mean_squared_error: 0.9447\n",
      "Epoch 115/150\n",
      "100/100 [==============================] - 0s 619us/step - loss: 0.9443 - mean_squared_error: 0.9443\n",
      "Epoch 116/150\n",
      "100/100 [==============================] - 0s 625us/step - loss: 0.9456 - mean_squared_error: 0.9456\n",
      "Epoch 117/150\n",
      "100/100 [==============================] - 0s 624us/step - loss: 0.9451 - mean_squared_error: 0.9451\n",
      "Epoch 118/150\n",
      "100/100 [==============================] - 0s 680us/step - loss: 0.9464 - mean_squared_error: 0.9464\n",
      "Epoch 119/150\n",
      "100/100 [==============================] - 0s 646us/step - loss: 0.9452 - mean_squared_error: 0.9452\n",
      "Epoch 120/150\n",
      "100/100 [==============================] - 0s 654us/step - loss: 0.9439 - mean_squared_error: 0.9439\n",
      "Epoch 121/150\n",
      "100/100 [==============================] - 0s 633us/step - loss: 0.9445 - mean_squared_error: 0.9445\n",
      "Epoch 122/150\n",
      "100/100 [==============================] - 0s 638us/step - loss: 0.9466 - mean_squared_error: 0.9466\n",
      "Epoch 123/150\n",
      "100/100 [==============================] - 0s 655us/step - loss: 0.9437 - mean_squared_error: 0.9437\n",
      "Epoch 124/150\n",
      "100/100 [==============================] - 0s 700us/step - loss: 0.9447 - mean_squared_error: 0.9447\n",
      "Epoch 125/150\n",
      "100/100 [==============================] - 0s 670us/step - loss: 0.9448 - mean_squared_error: 0.9448\n",
      "Epoch 126/150\n",
      "100/100 [==============================] - 0s 705us/step - loss: 0.9439 - mean_squared_error: 0.9439\n",
      "Epoch 127/150\n",
      "100/100 [==============================] - 0s 654us/step - loss: 0.9445 - mean_squared_error: 0.9445\n",
      "Epoch 128/150\n",
      "100/100 [==============================] - 0s 666us/step - loss: 0.9457 - mean_squared_error: 0.9457\n",
      "Epoch 129/150\n",
      "100/100 [==============================] - 0s 675us/step - loss: 0.9431 - mean_squared_error: 0.9431\n",
      "Epoch 130/150\n",
      "100/100 [==============================] - 0s 704us/step - loss: 0.9447 - mean_squared_error: 0.9447\n",
      "Epoch 131/150\n",
      "100/100 [==============================] - 0s 664us/step - loss: 0.9447 - mean_squared_error: 0.9447\n",
      "Epoch 132/150\n",
      "100/100 [==============================] - 0s 670us/step - loss: 0.9468 - mean_squared_error: 0.9468\n",
      "Epoch 133/150\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.9431 - mean_squared_error: 0.9431\n",
      "Epoch 134/150\n",
      "100/100 [==============================] - 0s 621us/step - loss: 0.9453 - mean_squared_error: 0.9453\n",
      "Epoch 135/150\n",
      "100/100 [==============================] - 0s 643us/step - loss: 0.9451 - mean_squared_error: 0.9451\n",
      "Epoch 136/150\n",
      "100/100 [==============================] - 0s 640us/step - loss: 0.9465 - mean_squared_error: 0.9465\n",
      "Epoch 137/150\n",
      "100/100 [==============================] - 0s 648us/step - loss: 0.9443 - mean_squared_error: 0.9443\n",
      "Epoch 138/150\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.9460 - mean_squared_error: 0.9460\n",
      "Epoch 139/150\n",
      "100/100 [==============================] - 0s 628us/step - loss: 0.9477 - mean_squared_error: 0.9477\n",
      "Epoch 140/150\n",
      "100/100 [==============================] - 0s 673us/step - loss: 0.9444 - mean_squared_error: 0.9444\n",
      "Epoch 141/150\n",
      "100/100 [==============================] - 0s 634us/step - loss: 0.9476 - mean_squared_error: 0.9476\n",
      "Epoch 142/150\n",
      "100/100 [==============================] - 0s 629us/step - loss: 0.9454 - mean_squared_error: 0.9454\n",
      "Epoch 143/150\n",
      "100/100 [==============================] - 0s 618us/step - loss: 0.9436 - mean_squared_error: 0.9436\n",
      "Epoch 144/150\n",
      "100/100 [==============================] - 0s 590us/step - loss: 0.9444 - mean_squared_error: 0.9444\n",
      "Epoch 145/150\n",
      "100/100 [==============================] - 0s 677us/step - loss: 0.9456 - mean_squared_error: 0.9456\n",
      "Epoch 146/150\n",
      "100/100 [==============================] - 0s 747us/step - loss: 0.9443 - mean_squared_error: 0.9443\n",
      "Epoch 147/150\n",
      "100/100 [==============================] - 0s 729us/step - loss: 0.9437 - mean_squared_error: 0.9437\n",
      "Epoch 148/150\n",
      "100/100 [==============================] - 0s 671us/step - loss: 0.9451 - mean_squared_error: 0.9451\n",
      "Epoch 149/150\n",
      "100/100 [==============================] - 0s 643us/step - loss: 0.9445 - mean_squared_error: 0.9445\n",
      "Epoch 150/150\n",
      "100/100 [==============================] - 0s 682us/step - loss: 0.9431 - mean_squared_error: 0.9431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b9c0cc048>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X, B, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.14180516e-02,  2.47345380e-02, -1.15890019e-02, -2.03124061e-03,\n",
       "        1.07470915e-01, -3.23055118e-01, -1.50413066e-02, -7.92838484e-02,\n",
       "       -1.40724853e-01,  5.30860946e-03, -1.01981834e-01, -1.53320916e-02,\n",
       "        6.08446412e-02, -3.13251726e-02, -1.51612237e-02,  6.44705892e-02,\n",
       "        4.61843237e-03, -1.09218284e-02,  4.97352704e-03, -2.98522152e-02,\n",
       "       -1.46527104e-02, -1.79014988e-02,  2.46982619e-01,  1.06884480e-01,\n",
       "        2.65546851e-02,  1.30092189e-01,  1.03865296e-01,  9.78301093e-03,\n",
       "       -2.55147628e-02,  5.99350780e-04,  1.12908147e-02, -1.36290506e-01,\n",
       "        1.33417957e-02, -2.45578177e-02, -7.73887783e-02, -9.02772397e-02,\n",
       "       -5.02265878e-02,  3.08751278e-02, -8.33608508e-02, -9.87681374e-03,\n",
       "       -3.33339088e-02,  7.07734600e-02,  5.20612635e-02, -4.25149709e-01,\n",
       "       -5.97749464e-02, -1.11680292e-02,  1.45510331e-01, -2.71519162e-02,\n",
       "       -1.95415206e-02, -7.74633139e-02,  1.21133655e-01,  7.72000700e-02,\n",
       "        1.44963674e-02,  1.22894309e-02, -6.87506795e-02,  1.57411210e-02,\n",
       "       -5.70485182e-02,  1.33726314e-01,  2.03793690e-01,  1.32850766e-01,\n",
       "       -8.87451693e-03,  8.94837826e-02,  9.91181582e-02, -7.70382956e-03,\n",
       "        5.87395616e-02, -7.12979883e-02, -2.53261715e-01,  2.32466273e-02,\n",
       "       -1.89338446e-01, -5.48266657e-02, -3.11828442e-02,  4.39541005e-02,\n",
       "        1.80026844e-01,  7.72252828e-02, -3.30389351e-01, -4.11201209e-01,\n",
       "        4.83998768e-02,  6.20066784e-02, -7.17093945e-02, -3.57089303e-02,\n",
       "        1.63596235e-02, -1.19489640e-01,  6.79876655e-02,  1.41008124e-01,\n",
       "        1.50215998e-01,  1.60642080e-02,  1.40621752e-01,  6.17438033e-02,\n",
       "        1.07252553e-01, -4.66105379e-02, -1.26928046e-01,  1.60947442e-04,\n",
       "       -3.16288881e-02, -2.64249928e-02,  8.80739391e-02,  1.90757319e-01,\n",
       "       -7.96659142e-02,  3.92761454e-03, -6.96662217e-02,  1.35326199e-02,\n",
       "       -1.60741024e-02, -7.33400404e-01,  8.24732706e-03, -2.95576341e-02,\n",
       "        1.35075197e-01, -2.77529150e-01,  5.16638570e-02,  1.76804736e-01,\n",
       "       -9.76975635e-03, -4.35883887e-02, -3.12244482e-02, -7.57765025e-02,\n",
       "        1.55353710e-01, -2.16297433e-03,  1.70763686e-01, -2.63477676e-02,\n",
       "       -1.54299334e-01, -7.59606808e-03, -1.43404938e-02,  1.71196520e-01,\n",
       "        1.67398863e-02,  1.85248062e-01, -1.35465696e-01,  6.13938160e-02,\n",
       "       -2.15870455e-01, -2.55311914e-02, -5.83430007e-03, -1.20798722e-01,\n",
       "       -6.99591190e-02, -1.77671000e-01,  1.28054880e-02,  8.98720920e-02,\n",
       "       -4.34555113e-05, -5.83422594e-02, -2.84233361e-01, -6.25683665e-02,\n",
       "        1.73318498e-02, -7.30188638e-02,  4.82974462e-02, -1.47855461e-01,\n",
       "       -7.23719150e-02,  1.89302281e-01, -2.31418498e-02,  2.05091946e-02,\n",
       "        9.20718536e-03,  3.32757458e-03,  1.48419440e-01, -7.20784217e-02,\n",
       "        1.60722397e-02,  1.62836760e-01,  4.98943180e-02, -1.67672224e-02,\n",
       "        1.25246078e-01,  3.46857421e-02,  1.91450730e-01,  5.78611307e-02,\n",
       "        9.88117158e-02, -8.29714239e-02, -2.65641548e-02,  4.91227955e-04,\n",
       "        9.98911113e-02, -1.44446194e-02, -1.31900497e-02, -2.83332206e-02,\n",
       "        4.72234562e-03,  3.05587463e-02,  5.18960468e-02,  5.01600243e-02,\n",
       "       -7.26958066e-02, -7.25924075e-02, -8.93527642e-03, -3.60351175e-01,\n",
       "        5.99600188e-02,  1.35693960e-02, -7.47058168e-03,  1.24631450e-03,\n",
       "       -7.53258318e-02, -5.31532243e-03, -2.36657150e-02,  1.34140514e-02,\n",
       "       -1.39436066e-01, -7.54840672e-02, -9.86544043e-02, -5.42013884e-01,\n",
       "       -4.20310162e-02,  2.24511437e-02, -3.14673521e-02, -1.63561031e-02,\n",
       "        9.67926830e-02, -8.18226486e-02,  1.15849420e-01, -2.17337795e-02,\n",
       "       -3.16443630e-02, -1.17234170e-01,  1.89539030e-01,  4.81859259e-02,\n",
       "       -1.53625742e-01,  7.02277943e-03, -5.03285937e-02,  2.12819502e-03,\n",
       "        1.83137149e-01, -7.08360225e-04, -7.42236972e-02, -1.74104758e-02,\n",
       "       -4.64521833e-02,  1.12442538e-01, -9.35669690e-02,  1.31301284e-01,\n",
       "       -3.14153917e-02, -4.49097939e-02,  1.45657212e-01,  8.18951055e-03,\n",
       "       -1.82236768e-02,  1.79795660e-02,  1.59928761e-02,  7.30694458e-03,\n",
       "       -3.17089297e-02, -7.48894811e-02, -2.24358998e-02,  9.25777480e-03,\n",
       "        4.44450788e-02,  7.26564378e-02,  6.93260431e-02,  1.29633486e-01,\n",
       "       -2.19453983e-02,  1.57470964e-02,  1.17350034e-02, -6.94739819e-02,\n",
       "        6.59769028e-02, -8.16826895e-03,  1.64487630e-01, -9.82623249e-02,\n",
       "        4.23293561e-04, -3.15389447e-02, -3.12703289e-02,  5.74122742e-03,\n",
       "        1.86866336e-02, -1.31444529e-01,  7.24150464e-02,  5.84978238e-02,\n",
       "       -3.14831324e-02,  1.34836249e-02,  1.40579060e-01,  6.77437633e-02,\n",
       "        7.27778971e-02, -7.26562291e-02, -2.34152861e-02, -1.55156150e-01,\n",
       "       -1.14766024e-02,  1.70443013e-01,  1.70261025e-01, -4.58952971e-02,\n",
       "       -4.02631201e-02, -1.90702900e-02,  5.17971441e-03, -2.87781544e-02,\n",
       "       -8.13301280e-03,  1.20712340e-01,  9.59702879e-02, -2.06065811e-02,\n",
       "        1.46298222e-02, -7.19915181e-02, -7.60574937e-02,  7.84189254e-02,\n",
       "        2.10358016e-02, -3.32805842e-01, -7.28528649e-02, -1.53742924e-01,\n",
       "       -4.96536866e-03, -7.17828572e-02,  1.83215275e-01,  1.92668244e-01,\n",
       "       -1.64826289e-01,  1.34207904e-01, -2.83684321e-02, -1.27861202e-02,\n",
       "        1.36027679e-01,  2.30905414e-02, -5.65775000e-02,  1.25392638e-02,\n",
       "        6.60591722e-02,  1.84473813e-01, -7.77881220e-03, -2.78436273e-01,\n",
       "        3.43625955e-02,  7.97116756e-02,  6.37641475e-02,  1.18119679e-02,\n",
       "       -1.70411676e-01, -1.82082564e-01, -4.66995746e-01, -3.16168927e-02,\n",
       "        1.23262666e-02, -1.12015195e-02, -1.38028696e-01,  2.63055116e-02,\n",
       "        1.63180336e-01,  2.83520631e-02,  1.45277977e-01,  6.47912621e-02,\n",
       "       -1.00906447e-01,  1.92162737e-01, -3.14055718e-02, -3.14589553e-02,\n",
       "        6.15634136e-02, -3.12419645e-02, -8.68767351e-02, -2.84651481e-02,\n",
       "       -7.28087425e-02,  1.94250360e-01, -5.91968484e-02, -2.69091547e-01,\n",
       "       -7.60795623e-02, -1.54613152e-01, -2.99120277e-01, -3.12760659e-02,\n",
       "        3.16777453e-03,  6.00904636e-02,  7.23462924e-03, -1.06324889e-02,\n",
       "        1.78810462e-01, -1.83660537e-04,  8.69675726e-02,  1.71805397e-01,\n",
       "        2.49820761e-02, -4.68023308e-02,  6.65571541e-04, -7.60788918e-02,\n",
       "        1.32312998e-01, -4.47553582e-02, -9.53324512e-03, -1.20658427e-04,\n",
       "        9.61151272e-02, -7.22783357e-02,  1.61872916e-02,  3.01863067e-02,\n",
       "       -2.47722454e-02, -6.95224479e-03,  4.50500064e-02, -5.05261309e-02,\n",
       "        1.37040362e-01, -1.88526697e-02,  1.42136142e-01,  1.66810229e-01,\n",
       "       -3.15004103e-02,  1.56227261e-01,  9.00994614e-03, -1.62831508e-02,\n",
       "       -1.75522901e-02,  1.29939273e-01, -1.85056090e-01, -1.11973137e-02,\n",
       "        2.78921537e-02,  1.24616355e-01, -1.06004819e-01, -1.76813871e-01,\n",
       "        9.21355337e-02,  5.28986380e-03,  9.45937485e-02,  1.21605590e-01,\n",
       "        1.93744048e-01, -7.38442838e-02,  2.49367103e-01,  7.03058988e-02,\n",
       "       -3.98392230e-03,  1.39485858e-02, -3.13099734e-02, -7.92864338e-03,\n",
       "       -2.50020288e-02,  1.19779095e-01, -1.34410322e-01, -1.81054935e-01,\n",
       "        7.39070922e-02, -1.37455523e-01,  1.02990732e-01, -3.67807634e-02,\n",
       "        2.04957649e-03, -1.27795525e-02,  1.89032599e-01,  3.79636474e-02,\n",
       "       -3.12521495e-02, -2.18475923e-01,  7.66203851e-02,  1.48737133e-01,\n",
       "        8.87152404e-02,  1.82905063e-01,  8.77325684e-02, -1.27905495e-02,\n",
       "        1.54294185e-02,  1.72901914e-01,  4.49948385e-03,  8.99110362e-03,\n",
       "       -1.83080137e-01, -2.04524741e-01,  7.27479756e-02, -7.56643713e-02,\n",
       "        1.27429605e-01, -6.68516755e-03, -2.58749992e-01,  6.71727657e-02,\n",
       "       -4.72221524e-04, -3.14615108e-02,  1.20238774e-02,  7.79469609e-02,\n",
       "       -3.97373177e-02, -1.38170198e-01, -2.73404233e-02, -4.96445328e-01,\n",
       "       -4.53907639e-01, -6.79780662e-01, -4.87468094e-01,  1.39638148e-02,\n",
       "        7.80914873e-02, -1.15512125e-02,  1.91979781e-01,  8.58167931e-03,\n",
       "        8.73435587e-02, -3.85101140e-01,  5.26104979e-02,  1.41990595e-02,\n",
       "       -9.73654538e-03, -9.21221450e-03, -2.11509705e-01, -3.15146632e-02,\n",
       "        1.74010783e-01,  7.57620856e-03,  1.57366306e-01, -5.70077449e-04,\n",
       "       -8.59303027e-03, -3.22272666e-02, -4.90961760e-01, -7.90935755e-02,\n",
       "       -7.15876818e-02,  2.99794488e-02, -3.38225439e-03,  3.73120420e-02,\n",
       "       -7.54822344e-02, -3.49612422e-02, -9.45349187e-02, -6.15159161e-02,\n",
       "        2.28572302e-02,  7.66006410e-02,  1.49502642e-02, -3.57014924e-01,\n",
       "        1.40341483e-02,  1.96256980e-01,  2.36604922e-02, -3.36418331e-01,\n",
       "       -1.24555722e-01, -1.60331056e-02, -8.86481851e-02, -1.69718698e-01,\n",
       "        1.01969793e-01,  8.38678330e-04,  8.69100839e-02, -1.37410715e-01,\n",
       "        3.80162820e-02, -1.55982152e-01, -1.45018220e-01, -3.12625654e-02,\n",
       "       -2.60730654e-01,  1.56302266e-02,  5.81845157e-02,  1.36996880e-01,\n",
       "        1.68991283e-01, -5.79696782e-02,  7.13743418e-02, -1.57046124e-01,\n",
       "        7.29411468e-03,  9.12228078e-02, -1.92939147e-01, -6.38288558e-02,\n",
       "        1.05809376e-01, -7.56262541e-02,  1.59716815e-01, -2.27718055e-03,\n",
       "        8.29046667e-02,  1.14008203e-01, -4.09317389e-03,  8.85914415e-02,\n",
       "        1.40767887e-01,  1.65035099e-01, -1.05531104e-02, -1.66857198e-01,\n",
       "        8.57913867e-03,  9.52728540e-02, -3.14029641e-02,  1.22048482e-01,\n",
       "       -7.44983852e-02, -1.68432631e-02, -1.00268796e-01,  9.23257321e-04,\n",
       "        1.73102133e-02,  1.58702545e-02, -7.66913146e-02,  1.29363500e-02,\n",
       "       -4.89622243e-02,  1.46797560e-02, -1.53093226e-02,  3.26228142e-02,\n",
       "        1.96793079e-02,  2.30387971e-03,  1.54164247e-02,  2.27654390e-02,\n",
       "       -1.09106138e-01,  4.55354005e-02,  1.83254674e-01, -7.73189068e-02,\n",
       "       -3.39738615e-02, -2.79942192e-02,  8.73948783e-02,  2.68463418e-02,\n",
       "        8.39996114e-02, -1.08171016e-01,  2.58918814e-02, -1.66445076e-01,\n",
       "       -1.64181478e-02, -1.21581331e-01, -9.12347138e-02, -7.14530349e-02,\n",
       "       -3.16680036e-02,  1.28276162e-02, -1.49174452e-01,  1.62936196e-01,\n",
       "       -2.74415873e-02,  2.99910828e-03, -2.76973903e-01, -1.82418540e-01,\n",
       "        1.23490110e-01, -1.61707476e-02, -3.84881720e-03,  2.71250717e-02,\n",
       "        8.71114992e-03, -1.61347277e-02, -3.14506926e-02,  1.29598647e-01,\n",
       "       -1.66250259e-01, -2.86407560e-01,  9.29618813e-03, -7.56338686e-02,\n",
       "       -1.22971907e-01, -1.66730173e-02,  3.48991565e-02,  2.45944969e-02,\n",
       "       -9.25001875e-03,  9.68697518e-02, -4.92013581e-02,  2.25458853e-02,\n",
       "        1.46908276e-02,  1.60168074e-02,  1.45248696e-03, -1.57634094e-02,\n",
       "        4.04157750e-02, -3.13998051e-02, -3.15113105e-02,  5.79273216e-02,\n",
       "        1.55426674e-02, -9.84415561e-02,  7.31550604e-02,  1.91325657e-02,\n",
       "       -2.40390912e-01, -4.01039533e-02, -1.11379400e-02, -3.14761735e-02,\n",
       "        1.42770842e-01, -3.53021808e-02, -3.15642394e-02, -7.57092088e-02,\n",
       "       -4.05698903e-02,  1.68505162e-01,  8.94923508e-02, -3.33271652e-01,\n",
       "       -8.84814411e-02,  2.84924321e-02,  1.49873033e-01,  9.17614624e-03,\n",
       "       -8.24771672e-02, -5.44840805e-02,  1.71360910e-01,  1.53530911e-01,\n",
       "       -1.33924671e-02,  3.48902754e-02, -1.51425414e-02, -3.16584349e-01,\n",
       "        5.80802374e-02, -2.64990687e-01, -1.62692256e-02, -1.93715431e-02,\n",
       "        7.60926753e-02,  6.88050836e-02, -1.41273439e-01, -4.59667258e-02,\n",
       "        1.08579025e-01,  2.08997391e-02,  3.49728875e-02,  1.95767507e-01,\n",
       "       -8.70765001e-02, -1.32445209e-02,  1.90813944e-01, -4.57554199e-02,\n",
       "        1.93058923e-02,  1.22287676e-01, -7.94369727e-02, -2.38679349e-05,\n",
       "       -6.47771731e-03, -3.44194025e-01, -7.79005885e-03,  2.82580048e-01,\n",
       "        4.99600172e-02, -4.42425199e-02, -8.75009745e-02,  7.88811296e-02,\n",
       "        1.49751566e-02,  7.31595308e-02,  1.24416932e-01, -1.89608112e-01,\n",
       "       -1.93058923e-01, -9.25953686e-03,  1.48758858e-01, -7.77389556e-02,\n",
       "       -9.09769908e-03,  1.69430301e-03, -5.86245991e-02, -5.44971645e-01,\n",
       "        1.76911205e-01,  3.85471694e-02, -1.60624273e-02, -1.40942819e-02,\n",
       "        1.06365085e-02,  1.96715638e-01, -2.61696614e-02,  8.25295895e-02,\n",
       "       -2.66222298e-01, -2.92911008e-03,  1.33983687e-01, -2.84655184e-01,\n",
       "       -2.04891898e-02, -8.03643093e-03, -8.51553231e-02, -1.18635111e-02,\n",
       "       -2.95401551e-02,  1.71614692e-01,  1.82641491e-01, -2.74515189e-02,\n",
       "       -6.02607764e-02, -1.27018586e-01,  1.51568785e-01,  1.71439856e-01,\n",
       "        3.50490622e-02, -8.54520053e-02,  7.81159997e-02,  1.84280112e-01,\n",
       "       -2.40042694e-02, -3.72866876e-02, -3.11846770e-02,  1.74947858e-01,\n",
       "       -9.06034857e-02, -2.58911401e-04, -4.20784682e-01, -5.77389784e-02,\n",
       "       -2.22795472e-01,  3.53664570e-02,  1.25712112e-01,  1.55158751e-02,\n",
       "        3.92235443e-03, -7.26267397e-02,  8.19804072e-02,  4.60597388e-02,\n",
       "       -1.05688736e-01,  1.01624325e-01, -1.44148804e-02,  1.96125433e-01,\n",
       "       -3.12923230e-02,  9.69434902e-03, -7.55380839e-02, -9.02849808e-03,\n",
       "       -2.77393498e-02, -6.34976476e-03,  2.52916627e-02, -7.23234415e-02,\n",
       "       -2.31648311e-01,  2.76762061e-02,  9.18097645e-02, -6.24406673e-02,\n",
       "       -1.21484473e-01, -3.15121152e-02, -3.14860679e-02,  1.48610905e-01,\n",
       "       -1.45476349e-02, -2.84311548e-03,  5.92490658e-02,  5.36273830e-02,\n",
       "        1.91299990e-03, -7.16099292e-02, -1.20970979e-02, -1.43302396e-01,\n",
       "        6.36034831e-03, -4.60051186e-02,  1.76049769e-01,  1.51607364e-01,\n",
       "        1.66740380e-02, -3.17101218e-02, -8.23311657e-02,  2.58636307e-02,\n",
       "       -1.91044025e-02,  7.30772130e-03, -3.14246528e-02,  1.07943222e-01,\n",
       "       -1.02309957e-01,  3.66814546e-02, -1.30276278e-01,  1.91478893e-01,\n",
       "       -1.83838196e-02,  1.39173977e-02,  1.09549686e-01,  1.47414915e-02,\n",
       "       -1.40613377e-01, -1.15176626e-02,  9.00666863e-02, -7.63431787e-02,\n",
       "       -1.24708578e-01, -1.53824106e-01, -1.55207410e-01, -7.45689571e-02,\n",
       "        8.19541663e-02, -1.00143313e-01, -7.29584396e-02,  6.90489635e-03,\n",
       "        1.39815174e-02, -7.01763779e-02,  8.68811831e-03,  5.43178432e-02,\n",
       "       -7.45543540e-02,  2.73209065e-04,  8.88561159e-02, -3.18646803e-03,\n",
       "       -6.66609854e-02, -5.92901297e-02,  1.16837732e-02,  1.41300142e-01,\n",
       "        2.42186598e-02,  4.49776463e-02,  1.13427460e-01, -1.21507421e-03,\n",
       "       -1.19452909e-01,  1.03401318e-01,  3.88599560e-03,  1.06398508e-01,\n",
       "        2.50622146e-02, -2.02679947e-01, -8.06807429e-02,  1.66522078e-02,\n",
       "        1.62913129e-01,  1.94902360e-01, -3.12660895e-02,  4.82609384e-02,\n",
       "        1.01641938e-03,  1.24183074e-02, -4.36921678e-02, -1.05271295e-01,\n",
       "        6.31560385e-03,  1.55877218e-01, -1.51332058e-02,  2.22699754e-02,\n",
       "       -1.24019720e-02, -1.53288618e-01, -1.40143231e-01, -1.54245958e-01,\n",
       "       -7.89006799e-02,  1.57396853e-01,  1.02228194e-01,  3.24538462e-02,\n",
       "       -3.45975399e-01,  1.28301442e-01,  1.93855137e-01,  4.29568104e-02,\n",
       "       -6.75000995e-02,  1.20922342e-01,  7.58868307e-02, -1.36440508e-02,\n",
       "       -2.54990794e-02, -3.13446559e-02, -5.87783717e-02,  7.93318003e-02,\n",
       "       -5.00744954e-03,  6.65981881e-03, -5.90281822e-02,  5.61686270e-02,\n",
       "        1.87507719e-01, -3.15243341e-02,  6.55238703e-03, -2.08311900e-01,\n",
       "       -1.45494007e-02,  3.17104682e-02, -1.14149585e-01, -1.11724809e-03,\n",
       "        9.10001248e-02, -1.89401396e-02,  2.08616070e-02,  1.03077784e-01,\n",
       "       -8.77812877e-03, -2.65281439e-01, -1.05430081e-01, -3.80522795e-02,\n",
       "       -1.28404088e-02,  3.92544083e-02,  1.84889525e-01,  1.15709528e-01,\n",
       "       -5.96600398e-03,  5.71769848e-03, -3.45113099e-01,  5.54172844e-02,\n",
       "        2.17504986e-02, -1.01585075e-01, -1.80148467e-01,  9.12285298e-02,\n",
       "       -1.25750341e-02,  6.23627417e-02,  1.28672831e-02, -7.15936720e-02,\n",
       "       -4.93942574e-03,  1.48906454e-01,  1.25729665e-01,  1.01639442e-02,\n",
       "        1.41892955e-03, -1.44170269e-01,  1.79782525e-01, -3.16965170e-02,\n",
       "        5.11545502e-02, -1.49290755e-01, -6.19916655e-02, -3.79898138e-02,\n",
       "        1.14959791e-01,  1.32096373e-02, -3.14168520e-02, -1.40298195e-02,\n",
       "       -1.39793277e-01, -3.15758772e-02,  8.41201395e-02, -6.83427602e-03,\n",
       "       -7.56696016e-02,  1.56852715e-02, -2.74563171e-02,  1.09855346e-02,\n",
       "       -3.15237604e-02,  1.41689442e-02, -3.69768590e-04, -8.88620690e-03,\n",
       "        7.27067441e-02, -2.62021452e-01,  1.55179575e-01,  3.39592509e-02,\n",
       "        1.49263851e-02,  1.62023865e-02, -1.33585818e-02, -1.55763745e-01,\n",
       "        1.15437321e-02,  1.19119540e-01,  5.13422303e-02, -3.16892974e-02,\n",
       "       -1.93414181e-01,  1.96393833e-01,  8.83993953e-02, -1.39923654e-02,\n",
       "        8.40057582e-02, -2.08385482e-01, -8.21699053e-02,  3.61228399e-02,\n",
       "        4.59394976e-03, -2.39297785e-02, -1.66285083e-01,  2.61632614e-02,\n",
       "       -1.19831525e-02, -9.10886675e-02, -6.90253451e-03, -3.14157866e-02,\n",
       "       -2.08127163e-02,  1.73159495e-01, -1.48295499e-02, -7.81755596e-02,\n",
       "       -3.15004773e-02, -1.27768107e-02, -1.24934167e-02, -3.13292183e-02,\n",
       "        1.13207810e-02, -3.44747156e-01, -4.42450494e-03,  5.37087731e-02,\n",
       "       -2.23062523e-02,  3.73543054e-03, -3.47795248e-01, -3.15405093e-02,\n",
       "        1.57279119e-01,  1.13699347e-01,  4.98647206e-02,  1.94913194e-01,\n",
       "        1.31195448e-02,  1.02387406e-02, -1.47580311e-01, -7.62973577e-02,\n",
       "       -5.97571209e-03, -3.12847756e-02, -1.44491866e-01, -6.11892760e-01,\n",
       "        1.23110637e-01, -7.48110712e-02, -2.99956277e-03, -2.40411952e-01,\n",
       "       -1.22807249e-01,  1.54457107e-01,  7.30306096e-03, -1.49590611e-01,\n",
       "        8.47009942e-03,  1.19077265e-01, -1.08993635e-01, -7.81239942e-03,\n",
       "       -1.86792135e-01, -7.63823390e-02,  7.42381439e-03, -7.23013431e-02,\n",
       "        4.53656726e-02, -7.73037523e-02, -2.43198648e-02,  1.16991550e-02,\n",
       "        5.14171831e-03,  7.20326975e-03, -1.50140807e-01,  1.90841094e-01,\n",
       "        7.65943974e-02, -6.55888170e-02, -8.35796818e-03, -7.66313970e-02,\n",
       "        1.51602611e-01,  1.22461431e-02, -4.67394665e-03,  2.03140117e-02,\n",
       "        5.70279956e-02,  4.37804274e-02, -3.84080678e-01, -7.10733980e-02,\n",
       "       -1.22650228e-02, -4.30945866e-02, -1.34762779e-01, -3.16659920e-02,\n",
       "       -3.16099636e-02, -1.41191185e-02, -1.61556937e-02, -1.64424255e-03,\n",
       "       -7.43286461e-02, -1.50348380e-01, -7.34570473e-02,  1.24327838e-03,\n",
       "       -4.35210653e-02, -9.02158171e-02, -9.31282341e-02, -9.86906290e-02,\n",
       "        7.77148232e-02,  1.83232017e-02, -1.24813989e-01, -1.15976110e-01,\n",
       "        1.82811432e-02,  7.34887868e-02,  8.91914219e-02, -7.43053854e-02,\n",
       "        1.56243607e-01, -1.09469481e-02,  4.32343371e-02,  5.84328920e-02,\n",
       "       -3.09111655e-01, -2.15044618e-03,  8.31219107e-02,  4.27227467e-04,\n",
       "        1.43685892e-01, -4.43812795e-02, -4.55268100e-03, -7.34099746e-02,\n",
       "       -7.21568614e-02,  1.30121842e-01, -1.22455239e-01, -4.65628095e-02,\n",
       "       -2.32487805e-02, -4.97683771e-02, -7.56468475e-02, -4.14592534e-01,\n",
       "       -4.11564410e-01,  1.61661431e-01, -3.11536677e-02, -1.48358479e-01,\n",
       "        1.81458697e-01, -7.84984529e-02,  1.02806501e-02,  8.73057693e-02,\n",
       "        3.67756300e-02, -7.98051432e-03,  1.45779215e-02, -6.05653226e-03,\n",
       "        1.83748648e-01,  6.60835579e-03, -1.78216361e-02,  2.67926417e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.scatter(A,B,c=L)\n",
    "# plt.scatter(A,model.predict(X_ones))\n",
    "#z=model.predict(X)\n",
    "#z.reshape((len(z,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=np.vstack((L,A)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ones=np.vstack((L_ones,A)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_ones=np.ones((L.shape))\n",
    "# # #print(L_ones)\n",
    "# L_minus=np.ones((L.shape))*leftside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_zero=np.vstack((L_minus,A)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressor.fit(X, B_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = regressor.predict([[0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_strat=stratifydata(L,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(A, B_strat,c=L)\n",
    "# plt.scatter(A, regressor.predict(X_zero), color = 'blue',label='L=0')\n",
    "# plt.scatter(A,regressor.predict(X_one),color='red',label='L=1')\n",
    "# plt.title('Stratitifed data (SVR)')\n",
    "# plt.xlabel('A with L ')\n",
    "# plt.ylabel('B')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predicting the target values of the test set\n",
    "# # y_pred = model.predict(X)\n",
    "\n",
    "# # RMSE (Root Mean Square Error)\n",
    "# rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)), '.3f'))\n",
    "# print(\"\\nRMSE: \", rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
